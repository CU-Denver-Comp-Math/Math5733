{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Two-Point BVPs\n",
    "---\n",
    "\n",
    "## Verification and Exploration of Numerical Content\n",
    "\n",
    "This notebook allows us to verify and explore, with some additional details, the content from Sections 2.2 through 2.4 in the text.\n",
    "\n",
    "It should be used as a companion, not a replacement, to a careful reading of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: A Finite Difference Approximation\n",
    "\n",
    "Starting with (2.14), we have that $A\\in\\mathbb{R}^{n\\times n}$ is given by\n",
    "\n",
    "$$\n",
    "\\large A = \\left(\n",
    "                \\begin{array}{ccccc}\n",
    "                    2 & -1 & 0 & \\cdots & 0 \\\\\n",
    "                    -1 & 2 & -1 & \\ddots & \\vdots \\\\\n",
    "                    0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "                    \\vdots & \\ddots & -1 & 2 & -1 \\\\\n",
    "                    0 & \\cdots & 0 & -1 & 2\n",
    "                \\end{array}\n",
    "           \\right)\n",
    "$$\n",
    "\n",
    "and the data vector is given by\n",
    "\n",
    "$$\n",
    "\\large b = h^2\\left(\n",
    "                    \\begin{array}{c} \n",
    "                        f(x_1) \\\\\n",
    "                        f(x_2) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        f(x_n)\n",
    "                    \\end{array}\n",
    "                \\right).\n",
    "$$\n",
    "\n",
    "- The $h^2$ comes from discretizing the differential operator with the centered finite difference scheme. \n",
    "\n",
    "  - It is perhaps slightly strange that we put this on the data vector instead of on $A$.\n",
    "\n",
    "  - Later, in Section 2.3, we see that $\\frac{1}{h^2}A$ defines the difference operator $L_h$. \n",
    "\n",
    "- The Gaussian elimination discussed in Section 2.2.3 should be familiar from undergraduate linear algebra.\n",
    "\n",
    "  - If not, it is straightforward if not a bit messy (you may want to review this here https://en.wikipedia.org/wiki/Gaussian_elimination). \n",
    "  \n",
    "  - The idea is to reduce the system of linear equations into row-echelon form (https://en.wikipedia.org/wiki/Row_echelon_form) and use back-substitution to then determine the vector $v$ such that $Av=b$. \n",
    "\n",
    "**Key question:** Since formally we can write $v=A^{-1}b$ (assuming $A^{-1}$ exists), why we do not simply just construct the inverse of the matrix?\n",
    "\n",
    "**The answer:** Because that is ***expensive*** in terms of FLOPS (https://en.wikipedia.org/wiki/FLOPS) and thanks to Gaussian elimination, completely unnecessary in determining $v$ with less FLOPS. \n",
    "\n",
    "**The algorithm:** While you could code up your own version of Gaussian elimination, or even just Algorithm 2.1 in the text (as you have to in Exercise 2.11), there are many available computational libraries that will do this for you efficiently. \n",
    "We show how using [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html) below on Example 2.5 from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup A\n",
    "n = 5  \n",
    "\n",
    "def make_A(n):\n",
    "    A = np.zeros((n,n))\n",
    "    np.fill_diagonal(A,2)\n",
    "    A += np.diag(-np.ones(n-1),k=1)\n",
    "    A += np.diag(-np.ones(n-1),k=-1)\n",
    "    return A\n",
    "\n",
    "A = make_A(n)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize domain and construct data vector \n",
    "x = np.linspace(0,1,n+2)\n",
    "h = x[1]-x[0]\n",
    "b = h**2 * (3*x+x**2)*np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.zeros(n+2)\n",
    "v[1:-1] = np.linalg.solve(A, b[1:-1])  # Numerical soln. using Gaussian elimination\n",
    "\n",
    "u = x*(1-x)*np.exp(x)  # Exact soln.\n",
    "\n",
    "%matplotlib widget\n",
    "plt.figure(1)\n",
    "plt.plot(x,v,label='Num. Soln. $v$')\n",
    "plt.plot(x,u,label='Exact Soln. $u$')\n",
    "plt.legend(loc='upper left', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore ROC\n",
    "ns = [5, 10, 20, 40, 80]\n",
    "E_h = np.zeros(5)\n",
    "h = np.zeros(5)\n",
    "alpha_h = np.zeros(5)\n",
    "\n",
    "count = 0\n",
    "# Header for table output in format useful for copy/paste into LaTeX\n",
    "print('  $n$   &   $h$    &     $E_h$    &   Rate of Conv. ' + r'\\\\ \\hline')\n",
    "for n in ns:\n",
    "    A = make_A(n)\n",
    "    \n",
    "    x = np.linspace(0,1,n+2)\n",
    "    h[count] = x[1]-x[0]\n",
    "    b = h[count]**2 * (3*x+x**2)*np.exp(x)\n",
    "\n",
    "    v = np.zeros(n+2)\n",
    "    v[1:-1] = np.linalg.solve(A, b[1:-1])  # Numerical soln. using Gaussian elimination\n",
    "\n",
    "    u = x*(1-x)*np.exp(x)  # Exact soln.\n",
    "    \n",
    "    E_h[count] = np.max(np.abs(u-v))\n",
    "    \n",
    "    if count == 0:\n",
    "        print(' {:3d}    &   {:.3f}  &  {:.3e}   &         '.format(n, h[count], E_h[count]) + r'   \\\\')\n",
    "    else:\n",
    "        alpha_h[count] = np.log(E_h[count]/E_h[count-1])/np.log(h[count]/h[count-1])\n",
    "        print(' {:3d}    &   {:.3f}  &  {:.3e}   &   {:.4f} '.format(n, h[count], E_h[count], alpha_h[count]) + r'  \\\\')\n",
    "    \n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: Continuous and Discrete Solutions\n",
    "\n",
    "Let's define a funciton for computing the discrete inner product for functions in $D_h$ like in (2.29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_h(u,v,h):\n",
    "    z = h * (u[0]*v[0] + u[-1]*v[-1])/2.0 + h*np.dot(u[1:-1],v[1:-1])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.20:\n",
    "\n",
    "Prove that if $u,v\\in C_0^2((0,1))$ then \n",
    "$$\n",
    "\\large    \\left| \\left< u, v \\right> -  \\left< u, v \\right>_h \\right| \\leq \\frac{h^2}{12} \\left|\\left| (uv)'' \\right|\\right|_\\infty\n",
    "$$\n",
    "\n",
    "In the proof below, we use the well established [error bound](https://en.wikipedia.org/wiki/Trapezoidal_rule#Error_analysis) associated with the [Trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule).\n",
    "\n",
    "***Proof:***\n",
    "\n",
    "Let $u,v\\in C_0^2((0,1))$. \n",
    "Then, $uv\\in C_0^2((0,1))$.\n",
    "\n",
    "Let $f=uv$. \n",
    "Then, $\\int_0^1 f(x)\\, dx = \\left<u,v\\right>$ and $\\left<u,v\\right>_h$ is identified as the trapezoidal rule applied to this integral.\n",
    "The result follows immediately from the established error bound for the trapezoidal rule. $\\Box$\n",
    "\n",
    "***Some extra material***\n",
    "\n",
    "If you've never seen the proof of the error bound for the trapezoidal rule, it follows from applying the integration-by-parts formula choice with some creative choices of integration constants.\n",
    "I will sketch out the process as a two-step process below that you may find more useful than the Wiki reference linked to above.\n",
    "\n",
    "(1) Lemma: For $f\\in C^2((0,1))$, let $x_i\\in[0,1)$ and $h>0$ sufficiently small so that $x_i+h\\in[0,1]$\n",
    "\n",
    "$$\n",
    "\\large \\left|\\int_{x_i}^{x_i+h} f(x)\\, dx - \\frac{h}{2}(f(x_i+h)-f(x_i)) \\right| \\leq \\frac{h^3}{12}\\left|\\left|  f''\\right|\\right|_\\infty.\n",
    "$$\n",
    "\n",
    "Sketch of Proof: A simple change of variables (to simplify the limits of integration) followed by integrating-by-parts twice with clever choices of integration constants leads to\n",
    "\n",
    "$$\n",
    " \\large    \\int_{x_i}^{x_i+h} f(x)\\, dx = \\int_0^h f(t+x_i)\\, dt = \\underbrace{\\frac{h}{2}\\left[f(x_i) + f(x_i+h)\\right]}_{\\text{Trap. Rule}} + \\underbrace{\\int_{0}^{h} \\left(\\frac{(t-h/2)^2}{2}-\\frac{h^2}{8} \\right)f''(t+x_i)\\, dt}_{\\text{Error in Trap. Rule on $(x_i, x_i+h)$}}.\n",
    "$$\n",
    "\n",
    "So it follows that the error can be bounded by\n",
    "\n",
    "$$\n",
    " \\large \\left| \\int_{0}^{h} \\left(\\frac{(t-h/2)^2}{2}-\\frac{h^2}{8} \\right)f''(t+x_i)\\, dt \\right| \\leq \\left|\\left|f''\\right|\\right|_\\infty \\int_0^h \\left| \\frac{(t-h/2)^2}{2}-\\frac{h^2}{8} \\right| \\, dt. \n",
    "$$\n",
    "\n",
    "The integrand is the absolute value of a parabola $\\displaystyle \\frac{(t-h/2)^2}{2}-\\frac{h^2}{8}$ which opens upward and is zero whenever $t-h/2 = \\pm h/2$ (i.e., at $t=0$ and at $t=h$), so for $t\\in(0,h)$, we have that\n",
    "\n",
    "$$\n",
    "   \\large \\left|\\frac{(t-h/2)^2}{2}-\\frac{h^2}{8}\\right| = \\frac{h^2}{8} - \\frac{(t-h/2)^2}{2}. \n",
    "$$\n",
    "\n",
    "It follows from a direct integration of this term that the error in the trapezoidal rule on $(x_i,x_i+h)$ is bounded by\n",
    "\n",
    "$$\n",
    " \\large \\left| \\int_{0}^{h} \\left(\\frac{(t-h/2)^2}{2}-\\frac{h^2}{8} \\right)f''(t+x_i)\\, dt \\right| \\leq \\frac{h^3}{12}\\left|\\left|f''\\right|\\right|_\\infty.\n",
    "$$\n",
    "\n",
    "(2) Since we apply the trapezoidal rule on $n$ subintervals, we add up the error bound $n$ times and use the fact that $h=1/n$ to get the result used in the proof of Exercise 2.20 above. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma 2.3 and 2.4: Symmetric Positive Defeniteness of the operator $L_h$\n",
    "\n",
    "The book provides one way of proving this. \n",
    "Here's another.\n",
    "\n",
    "For $u\\in D_{h,0}$, we observe that $L_h u$ is simply given by $\\frac{1}{h^2} A\\tilde{u}$ where $\\tilde{u}$ is the vector defined by evaluating $u$ at $x_i$ for $i=1,\\ldots, n$ and the matrix $A$ is as given above.\n",
    "Moreover, for $u,v\\in D_{h,0}$ we also have that $\\left<u,v\\right>_h$ simplifies to $h(\\tilde{u},\\tilde{v})$ where $(\\cdot,\\cdot)$ denotes the usual Euclidean inner product.\n",
    "Now, with this notation, and the fact that $A$ is a SPD matrix, the SPD of the operator $L_h$ follows immediately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A maximum principle for the discrete problem\n",
    "\n",
    "One take home message is that the discrete analog of the Dirac delta function is given by $\\frac{1}{h} e^k$ where $e^k\\in D_{h,0}$ is defined by $e^k(x_k)=1$ and $e^k(x_j)=0$ if $j\\neq k$.\n",
    "Implicit is the assumption that we also require that $1\\leq k\\leq n$, i.e., that we evaluate at an interior point since $e^k\\in D_{h,0}$ automatically implies that $e^k(x_0)=0=e^k(x_{n+1})$).\n",
    "\n",
    "Another take home message is that the discrete Green's function is given by $G^k(x_j) = G(x_j,x_k)$ where $G$ is the Green's function for the continuous problem so that $L_h G^k = \\frac{1}{h}e^k$. \n",
    "\n",
    "Let's explore this a bit numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(x,y): \n",
    "    if 0 <= y <= x:\n",
    "        z = y*(1-x)\n",
    "    else:\n",
    "        z = x*(1-y)\n",
    "    return z\n",
    "\n",
    "def make_G(n, x):    \n",
    "    G_k = np.zeros((n+2,n+2))\n",
    "    for j in range(0,n+2):\n",
    "        for k in range(0,n+2):\n",
    "            G_k[j,k] = G(x[j],x[k])\n",
    "    return G_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "x = np.linspace(0,1,n+2)\n",
    "h = x[1]-x[0]  # So 1/h = n+1\n",
    "\n",
    "G_k = make_G(n,x)\n",
    "        \n",
    "A = make_A(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an integer between 0 and n+1\n",
    "k = 2\n",
    "\n",
    "test = np.zeros(n+2)\n",
    "test[1:-1] = 1/h**2 * np.dot(A,G_k[1:-1,k])\n",
    "\n",
    "%matplotlib widget\n",
    "plt.figure(2) \n",
    "plt.plot(x,test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the inner product notation, we have for the continuous problem $u(x) = \\left< G(x,y),f(y) \\right>$ (where the integral is with respect to $y$ not $x$), and now for the discrete problem we also have that $v(x_j) = \\left< G^k(x_j), f \\right>_h$ (where the summation is with repect to $k$ not $j$).\n",
    "\n",
    "This implies another way for construction solutions using (2.33), which we explore numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to construct solutions using (2.33)\n",
    "\n",
    "b_old = h**2 * (3*x+x**2)*np.exp(x)\n",
    "\n",
    "v_old = np.zeros(n+2)\n",
    "v_old[1:-1] = np.linalg.solve(A, b_old[1:-1])  # Numerical soln. using Gaussian elimination\n",
    "\n",
    "b_new = (3*x+x**2)*np.exp(x)\n",
    "\n",
    "v_new = np.zeros(n+2)\n",
    "for j in range(1,n+1):\n",
    "    v_new[j] += inner_h(G_k[j,:],b_new,h)\n",
    "\n",
    "u = x*(1-x)*np.exp(x)  # Exact soln.\n",
    "\n",
    "%matplotlib widget\n",
    "plt.figure(3)\n",
    "plt.plot(x, v_old, 'b.', markersize=15, label='Num. Soln. $v_{old}$')\n",
    "plt.plot(x, v_new, 'g--', label='Num. Soln. $v_{new}$')\n",
    "plt.legend(loc='upper left', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure Proposition 2.6 holds for our numerically constructed discrete Green's functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = range(1,30)\n",
    "\n",
    "for n in ns:\n",
    "    x = np.linspace(0,1,n+2)\n",
    "    h = x[1]-x[0]  # So 1/h = n+1\n",
    "\n",
    "    G_k = make_G(n, x)\n",
    "    \n",
    "    temp = np.zeros(n+2)\n",
    "    for i in range(0,n+2):\n",
    "        temp[i] = inner_h(G_k[:,i], np.ones(n+2), h)\n",
    "        \n",
    "    print(np.max(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word of caution about discrete Green's functions.\n",
    "\n",
    "If we know $G$ for the continuous problem, then it appears as if determining the discrete Green's function is rather trivial and we can use it to easily determine solutions through simple computations of inner products.\n",
    "\n",
    "Formally, we think of $G$ as the inverse of the differential operator $L$ applied to $\\delta(x-y)$, and we can think of the discrete Green's function in a similar way as the inverse of $L_h$ applied to $e^k$. This simply means that the discrete Green's funciton may be constructed by determining $\\frac{1}{h^2}A^{-1}$ and then multipling this to the standard basis vectors (scaled by $1/h$) of $\\mathbb{R}^n$. \n",
    "\n",
    "However, this is a ***stupid*** computation because it involves inverting a matrix, which is really expensive and something we try to avoid whenever possible. \n",
    "\n",
    "Just like with regular Green's functions, which are really difficult to determine in general cases, the mere existence of one is usually enough to infer useful properties of the solution similar to how the existence of the inverse of a matrix is also useful even if we never construct it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4: Eigenvalue Problems\n",
    "\n",
    "Let's verify Lemma 2.9 numerically. First, we need to ***correct*** it since the book version contains an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:5px solid cyan\"> </hr>\n",
    "\n",
    "<span style=\"background-color:rgba(0, 255, 255, 0.5)\">Corrected Lemma:</span>\n",
    "\n",
    "The difference operator $L_h$ has eigenvalues\n",
    "\n",
    "$$\n",
    "    \\mu_k = \\frac{4}{h^2}\\sin^2(k\\pi h/2), \\qquad k=1,\\ldots, n.\n",
    "$$\n",
    "\n",
    "The corresponding eigenfunctions $v_k\\in D_{h,0}$ are given by\n",
    "\n",
    "$$\n",
    "    v_k = \\left(\n",
    "                \\begin{array}{c} \n",
    "                        \\sin(k\\pi x_1) \\\\\n",
    "                        \\sin(k\\pi x_2) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\sin(k\\pi x_n)\n",
    "                 \\end{array}\n",
    "           \\right).\n",
    "$$\n",
    "\n",
    "Furthermore, the discrete eigenfunctions are orthogonal and satisfy\n",
    "\n",
    "$$\\large\n",
    "    \\langle v_k, v_m \\rangle = \\begin{cases}\n",
    "                                    0, & k\\neq m, \\\\\n",
    "                                    \\frac{1}{2 \\color{red}{h}}, & k=m.\n",
    "                               \\end{cases}\n",
    "$$\n",
    "\n",
    "<hr style=\"border:5px solid cyan\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at plots of the numerically estimated eigenfunctions and their errors due to numerical approximation, we need to understand a few key points. \n",
    "\n",
    "<span style=\"background-color:rgba(255, 0, 255, 0.5)\">***Key points:***</span>\n",
    "\n",
    "- Using a linear algebra package to estimate the eigenvalues/eigenfunctions of a matrix will typically produce results that are normalized with respect to the Euclidean norm (i.e., the 2-norm).\n",
    "\n",
    "- This implies that the $v_k$ computed from a linear algebra package will likely have the property that $\\langle v_k, v_k \\rangle = 1$.\n",
    "\n",
    "- The sign of an eigenvector is completely arbitrary, and we could have just as easily stated that the discrete eigenfunctions were computed using $-\\sin$ in place of $\\sin$ in the lemma above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "x = np.linspace(0,1,n+2)\n",
    "h = x[1]-x[0]\n",
    "\n",
    "A = make_A(n)\n",
    "\n",
    "mu, v = np.linalg.eigh(1/h**2 * A)  # eigh returns eigs of symmetric matrix in increasing order\n",
    "\n",
    "# Lemma 2.9\n",
    "mu_exact = np.zeros(n)\n",
    "for k in range(0,n):\n",
    "    mu_exact[k] = 4/(h**2) * (np.sin((k+1)*np.pi*h/2))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu)\n",
    "print('-'*50) \n",
    "print(np.max(np.abs(mu-mu_exact)))  # Largest error made in estimating an eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_exact = np.zeros((n,n))\n",
    "for k in range(0,n):\n",
    "    v_exact[:,k] = np.sin((k+1)*np.pi*x[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the eig functions in numpy will normalize eigenvectors \n",
    "# with respect to the usual 2-norm and the sign is arbitrary\n",
    "\n",
    "# Choose k between 1 and 30\n",
    "k = 3\n",
    "\n",
    "print('Component #' + str(k) + ' should be 1 and the others\\n' +\\\n",
    "      'should be approximately zero for estimated eigenvectors')\n",
    "print('-'*50)\n",
    "print(v.T.dot(v)[k-1, :])\n",
    "\n",
    "print('\\n\\n')\n",
    "print('-'*50)\n",
    "print('Component #' + str(k) + ' should be ' + str(1/(2*h)) + ' and the others\\n' +\\\n",
    "      'should be approximately zero for eigenvectors from Lemma')\n",
    "print(v_exact.T.dot(v_exact)[k-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Pick k between 1 and 30\n",
    "k = 3\n",
    "\n",
    "ind = np.argmax(np.abs(v[:,k-1]))\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(x[1:-1],v[:,k-1]/v[ind,k-1])\n",
    "plt.title('$L^\\infty$ normalized estimated numerical e.func #' + str(k))\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(x[1:-1],v[:,k-1]/v[ind,k-1]-np.sin(k*np.pi*x[1:-1]))\n",
    "plt.title('Difference from exact to estimated numerical e.func #' + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_spectral(x, g, v, n, trunc, norm_const=1):\n",
    "    \n",
    "    g_sum = np.zeros(n+2)\n",
    "    \n",
    "    for k in range(trunc):\n",
    "        g_sum[1:-1] += np.dot(g[1:-1],v[:,k])*v[:,k] / norm_const\n",
    "    \n",
    "    return g_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = x*(1-x)*np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(6)\n",
    "plt.plot(x, g)\n",
    "plt.plot(x, g_spectral(x, g, v, n, trunc=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(7)\n",
    "plt.plot(x, g)\n",
    "plt.plot(x, g_spectral(x, g, v_exact, n, trunc=2, norm_const = v_exact[:,0].dot(v_exact[:,0])))\n",
    "plt.title('Normalizing constant used = ' + str(v_exact[:,0].dot(v_exact[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
