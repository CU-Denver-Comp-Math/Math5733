{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6de630-74b8-40a5-b0fb-c501a9f8ca39",
   "metadata": {},
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6099d48-8788-4c41-b7aa-ec020f51137a",
   "metadata": {},
   "source": [
    "## Section 2.4: Eigenvalue Problems\n",
    "---\n",
    "\n",
    "In this section, we consider continuous eigenvalue problems of the form\n",
    "\n",
    "$$\n",
    "    Lu = \\lambda u, \\ u\\in \\mathcal{C}^2_0((0,1))-\\{0\\}\n",
    "$$\n",
    "\n",
    "and discrete eigenvalue problems of the form\n",
    "\n",
    "$$\n",
    "    L_h v = \\lambda v, \\ v\\in D_{h,0}-\\{0\\}.\n",
    "$$\n",
    "\n",
    "In other words, we are considering the 2-pt BVPs presented in the operator form seen in the previous notebook where the forcing function has the form of being a scalar multiple, $\\lambda$, of the solution.\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- By \"$-\\{0\\}$\" we mean that we are looking for functions that are not identically equal to zero. \n",
    "\n",
    "- For a given $\\lambda\\in\\mathbb{C}$, if there exists a nonzero (continuous or discrete) function satisfying the above (continuous or discrete) eigenvalue problem, then we call such a $\\lambda$ an **eigenvalue** and the corresponding function an **eigenfunction.**\n",
    "\n",
    "  - We will show that any eigenvalues must actually be real (i.e., have zero imaginary component) due to the symmetry of the operators $L$ and $L_h$.\n",
    "\n",
    "\n",
    "- For a given eigenvalue $\\lambda\\in\\mathbb{R}$ (see the above remark), there is no unique way to represent the eigenfunction because any non-zero multiple of this eigenfunction also defines an eigenfunction associated with the eigenvalue. \n",
    "\n",
    "  In other words, if $u$ is a continuous eigenfunction associated with $\\lambda$, then so is $\\alpha u$ for any $\\alpha\\in\\mathbb{R}-\\{0\\}$, which is verified by direct substitution into the eigenvalue problem.\n",
    "  \n",
    "**Why are we studying this?**\n",
    "\n",
    "- This is related to Fourier series expansions that prove to be a useful tool in later chapters for solving PDEs.\n",
    "\n",
    "- Some aspects of modern PDE theory rely on analysis of the [spectrum](https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)) of the differential operator. The spectrum is a generalization of the concept of eigenvalues.\n",
    "\n",
    "- Analyzing the eigenvalues/spectrum of an operator gives us a lot of information about the solvability of problems and any conditions on the data that are required for a solution to exist. \n",
    "\n",
    "  - Consider the classic linear algebra $Av=\\lambda v$ eigenvalue problem (obviously $A$ is a square matrix - why is this obvious?). If $\\lambda=0$ is an eigenvalue, then it tells us that the nullspace of $A$ is non-empty. Moreover, this tells us that $Ax=b$ can only be solved for *some* $b$ and that for the $b$ we can choose for which a solution to $Ax=b$ exists, we also know that the solutions are not unique because we can add to these solutions any vector in the nullspace of $A$. The lesson is that linear algebra is a subject too often taken for granted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402cbd5-b14c-44f2-aec7-35f8a92c54bf",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.4.1'>Section 2.4.1: Immediate properties of eigenvalues and eigenfunctions for $L$ and $L_h$</a>\n",
    "---\n",
    "\n",
    "Recall from [Section 2.3](Chp2Sec.ipynb) the following lemma.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### Lemma 2.3.1: Symmetry of Operators\n",
    "\n",
    "The operators $L$ and $L_h$ are symmetric.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose $\\lambda\\in\\mathbb{C}$ is an eigenvalue for $L$ and consider the associated eigenfunction $u\\in\\mathcal{C}^2_0((0,1))$. By Lemma 2.3.1, \n",
    "\n",
    "$$\n",
    "    \\langle Lu, u\\rangle = \\langle u, Lu\\rangle.\n",
    "$$\n",
    "\n",
    "Of course, $Lu=\\lambda u$, so this means that\n",
    "\n",
    "$$\n",
    "    \\langle\\lambda u, u \\rangle = \\langle u, \\lambda u\\rangle.\n",
    "$$\n",
    "\n",
    "However, if we are taking inner products for complex-valued (instead of real-valued) vector spaces, then the inner product must have the property that for any two vectors $u$ and $v$ and complex number $\\alpha$, \n",
    "\n",
    "$$\n",
    "    \\langle \\alpha u, v\\rangle = \\alpha \\langle u, v \\rangle, \\ \\text{ and } \\ \\langle u, \\alpha v\\rangle = \\overline{\\alpha}\\langle u, v \\rangle\n",
    "$$\n",
    "\n",
    "where $\\overline{\\alpha}$ denotes the complex conjugate. This means that\n",
    "\n",
    "$$\n",
    "    \\langle\\lambda u, u \\rangle = \\langle u, \\lambda u\\rangle \\Rightarrow \\lambda \\langle u, u \\rangle = \\overline{\\lambda} \\langle u, u\\rangle.\n",
    "$$\n",
    "\n",
    "The above is only true if $\\lambda=\\overline{\\lambda}$, and the only complex numbers that are equal to their complex conjugates are those with zero imaginary part. In other words, the symmetry of the operator implies that if any eigenvalues exist, then they must be real-valued. \n",
    "\n",
    "A similar argument applies to the eigenvalues of $L_h$. We summarize these results in the following lemma.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### Lemma 2.4.1: Real-valued eigenvalues\n",
    "\n",
    "Any eigenvalues that exist for $L$ or $L_h$ are real-valued.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9551744-179f-4a6e-9c46-f93830b4391e",
   "metadata": {},
   "source": [
    "We now recall another lemma from [Section 2.3](Chp2Sec3.ipynb).\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### Lemma 2.3.2: Positive Defineteness of Operators\n",
    "\n",
    "The operators $L$ and $L_h$ are positive definite.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "From this lemma, we have that if $\\lambda$ is an eigenvalue for $L$ with the associated eigenfunction $u$, then because $u$ is nonzero, we have\n",
    "\n",
    "$$\n",
    "    \\langle Lu, u \\rangle > 0.\n",
    "$$\n",
    "\n",
    "On the other hand, \n",
    "\n",
    "$$\n",
    "    \\langle Lu, u \\rangle = \\lambda \\langle u, u\\rangle, \n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "    \\lambda \\langle u, u \\rangle >0.\n",
    "$$\n",
    "\n",
    "Since $\\langle u, u \\rangle>0$ (it is the integral of a nonnegative function that must be positive at least somewhere since the function is nonzero and continuous), it follows that $\\lambda>0$ as well.\n",
    "\n",
    "A similar argument applies to the eigenvalues of $L_h$. We summarize these results in the following lemma.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### Lemma 2.4.2: Positivity of eigenvalues\n",
    "\n",
    "Any eigenvalues that exist for $L$ or $L_h$ are positive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313eeff8-d3c5-4ce8-883c-375ca575e333",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db26336-8896-4031-8af6-32daf0267685",
   "metadata": {},
   "source": [
    "Recall that inner products impart a *geometric structure* onto a vector space. When the inner product between two vectors is zero, the vectors are said to be orthogonal. \n",
    "\n",
    "Suppose $\\lambda$ and $\\gamma$ are eigenvalues of $L$ such that $\\lambda\\neq \\gamma$, and let $u$ and $v$ denote the eigenfunctions associated with these eigenvalues, respectively.\n",
    "\n",
    "Then, by symmetry of $L$ and properties of inner products, we have\n",
    "\n",
    "$$\n",
    "    \\lambda\\langle u, v\\rangle = \\langle Lu, v \\rangle = \\langle u, Lv \\rangle = \\gamma \\langle u, v\\rangle.\n",
    "$$\n",
    "\n",
    "The only way that $\\lambda \\langle u, v \\rangle = \\gamma \\langle u, v\\rangle$ is if $\\langle u, v \\rangle =0$. Thus, the eigenfunctions associated with distinct eigenvalues are orthorgonal to each other. \n",
    "\n",
    "A similar argument applies to $L_h$. We summarize these results in the following lemma.\n",
    "\n",
    "---\n",
    "#### Lemma 2.4.3: Orthorgonality of eigenfunctions\n",
    "\n",
    "Eigenfunctions associated with distinct eigenvalues for $L$ or $L_h$ are orthogonal.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Lemmas 2.4.1 and 2.4.2 state that if $\\lambda$ is an eigenvalue for either $L$ or $L_h$, then necessarily $\\lambda\\in\\mathbb{R}$ and $\\lambda>0$. Lemma 2.4.3 states a geometry property of the eigenfunctions.**\n",
    "\n",
    "However, none of these results actually state that eigenvalues exist, what the values actually are, or what the associated eigenfunctions are! These are just statements of properties of things that may or may not exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2691de5-dd1e-473e-91e8-db9f1a2f2e2c",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.4.2'>Section 2.4.2: The eigenvalues and eigenfunctions of $L$</a>\n",
    "---\n",
    "\n",
    "It turns out that we can actually determine what the eigenvalues are for $L$ and $L_h$ because the associated eigenproblems are simple enough for us to analyze with basic ODE and linear algebra techniques. We just have to be clever enough to do so.\n",
    "\n",
    "For the continuous problem, suppose that $\\lambda$ is an eigenvalue. From Lemmas 2.4.1 and 2.4.2, we know that $\\lambda$ is some positive real number, i.e., $\\lambda>0$. This means we can take the square root to get another real number that we denote by $\\beta=\\sqrt{\\lambda}$.\n",
    "\n",
    "Why do we do this? Because it simplifies notation as we will see.\n",
    "\n",
    "Since $\\lambda=\\beta^2$, we have that the associated eigenfunction $u$ satisfies the following equation\n",
    "\n",
    "$$\n",
    "    -u'' = \\beta^2 u \\Rightarrow u'' + \\beta^2 u = 0.\n",
    "$$\n",
    "\n",
    "From elementary ODEs, we have that this second-order ODE has general solutions of the form\n",
    "\n",
    "$$\n",
    "    u(x) = c_1 \\cos(\\beta x) + c_2\\sin (\\beta x), \n",
    "$$\n",
    "\n",
    "where $c_1,c_2\\in\\mathbb{R}$. We now apply the first BC, \n",
    "\n",
    "$$\n",
    "    u(0) = 0 \\Rightarrow c_1 = 0.\n",
    "$$\n",
    "\n",
    "This means that the eigenfunction simplifies to the form\n",
    "\n",
    "$$\n",
    "    u(x) = c_2\\sin(\\beta x) = c_2\\sin(\\sqrt{\\lambda} x).\n",
    "$$\n",
    "\n",
    "Now, the other BC gives,\n",
    "\n",
    "$$\n",
    "    u(1) = 0 \\Rightarrow c_2\\sin(\\beta) = 0 \\Rightarrow c_2\\sin(\\sqrt{\\lambda}) = 0.\n",
    "$$\n",
    "\n",
    "Remember, $\\lambda$ can only be an eigenvalue if there is a nonzero eigenfunction, so we need to have $c_2\\neq 0$, and it does not matter what the value of $c_2$ is since we only ever determine an eigenfunction up to a multiplicative constant anyway. This means that we need\n",
    "\n",
    "$$\n",
    "    \\sin(\\beta)=0 = \\sin(\\sqrt{\\lambda}).\n",
    "$$\n",
    "\n",
    "Lucky for us, there are an infinite number of *positive* values of $\\beta$ (and thus of $\\lambda$) such that this is true, meaning that there are an *infinite number of eigenvalues associated with $L$*. Specifically, if\n",
    "\n",
    "$$\n",
    "    \\beta = k\\pi, \\ k\\in\\mathbb{N},\n",
    "$$\n",
    "\n",
    "meaning $\\beta$ is any positive integer multiple of $\\pi$, then $\\lambda=(k\\pi)^2$ defines an eigenvalue with eigenfunction given by $\\sin(k\\pi x)$. \n",
    "\n",
    "Notice that the eigenvalues are countably infinite. We therefore use an index to enumerate them as shown in the following lemma.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### Lemma 2.4.4: Eigenvalues and eigenfunctions of $L$\n",
    "\n",
    "The eigenvalues of $L$ are given by \n",
    "\n",
    "$$\n",
    "    \\lambda_k = (k\\pi)^2, \\ k\\in\\mathbb{N}, \n",
    "$$\n",
    "\n",
    "and the associated eigenfunctions are given by\n",
    "\n",
    "$$\n",
    "    u_k(x) = \\sin(k\\pi x), \\ k\\in\\mathbb{N}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "We already know from Lemma 2.4.3 that for $k, m\\in\\mathbb{N}$ with $k\\neq m$, the eigenfunctions $u_k$ and $u_m$ are orthogonal with respect to the continuous inner product. It is a good calculus exercise for students to directly verify via integration that this is the case by utilizing a [product to sum trigonometric identity](https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Product-to-sum_and_sum-to-product_identities) to rewrite the product of $\\sin(k\\pi x)\\sin(m\\pi x)$. \n",
    "\n",
    "In other words, if $k\\neq m$, then we have $\\langle u_k, u_m\\rangle =0$.\n",
    "\n",
    "What is the value of $\\langle u_k, u_k\\rangle$?\n",
    "\n",
    "Using either the same trigonomtric identity (or a double angle formula), students should show that $\\langle u_k, u_k\\rangle = \\frac{1}{2}$ for all $k\\in\\mathbb{N}$. \n",
    "\n",
    "We summarize this below.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### Lemma 2.4.5: Inner products of eigenfunctions of $L$\n",
    "\n",
    "For $k\\in\\mathbb{N}$, the eigenfunctions $u_k$ of $L$ given in Lemma 2.4.4 have the property that\n",
    "\n",
    "$$\n",
    "    \\langle u_k, u_m \\rangle = \\begin{cases}\n",
    "                                   0, & k\\neq m, \\\\\n",
    "                                   \\frac{1}{2}, & k=m.\n",
    "                               \\end{cases}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4668620-259f-4491-bf94-e4476428e4c8",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.4.3'>Section 2.4.3: The eigenvalues and eigenfunctions of $L_h$</a>\n",
    "---\n",
    "\n",
    "Since we know that $\\sin(\\beta x)$ (for suitable choices of $\\beta$) are eigenfunctions of $L$ and that $L_hu \\approx Lu$, we check via direct substitution to see if there are $\\beta$ that make $\\sin(\\beta x)$ an eigenfunction for $L_h$.\n",
    "\n",
    "**Step 1:** Substitution.\n",
    "\n",
    "$$\n",
    "    L_h \\sin(\\beta x) = \\frac{1}{h^2} \\left[ -\\sin(\\beta (x-h)) + 2\\sin(\\beta x) - \\sin(\\beta(x+h))\\right]\n",
    "$$\n",
    "\n",
    "**Step 2:** Utilize a [product to sum trigonometric identity](https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Product-to-sum_and_sum-to-product_identities) to write\n",
    "\n",
    "$$\n",
    "    - \\left[\\sin(\\beta (x-h)) + \\sin(\\beta(x+h))\\right] = -2\\sin(\\beta x)\\cos(-\\beta h). \n",
    "$$\n",
    "\n",
    "We use the evenness of cosine to rewrite this as\n",
    "\n",
    "$$\n",
    "    - \\left[\\sin(\\beta (x-h)) + \\sin(\\beta(x+h))\\right] = -2\\sin(\\beta x)\\cos(\\beta h).\n",
    "$$\n",
    "\n",
    "**Step 3:** Substituting what we have from Step 2 into what we have from Step 1 and factoring gives\n",
    "\n",
    "$$\n",
    "    L_h \\sin(\\beta x) = \\frac{2}{h^2}\\left[1-\\cos(\\beta h)\\right]\\sin(\\beta x).\n",
    "$$\n",
    "\n",
    "**Step 4:** Use a [half-angle formula](https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Half-angle_formulae) to write\n",
    "\n",
    "$$\n",
    "    1-\\cos(\\beta h) = 2\\sin^2(\\beta h/2).\n",
    "$$\n",
    "\n",
    "**Step 5:** Substitute what we have from Step 4 into what we have from Step 3 to give\n",
    "\n",
    "$$\n",
    "    L_h \\sin(\\beta x) = \\underbrace{\\frac{4}{h^2}\\sin^2(\\beta h/2)}_{\\text{Indep. of $x$}} \\sin(\\beta x)\n",
    "$$\n",
    "\n",
    "Observe that this implies $L_h \\sin(\\beta x)$ produces a constant multiple of $\\sin(\\beta x)$. \n",
    "\n",
    "We have not even specified $\\beta$ yet! Well, recall that to solve $L_h v = \\lambda v$, we still need $v\\in D_{h,0}$, so we need boundary conditions to hold. As before, the only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d608200-70bd-4f6b-a57e-46eca3fd3cd9",
   "metadata": {},
   "source": [
    "Recalling how $L_h$ is defined in [Section 2.3](Chp2Sec3.ipynb), we have that \n",
    "\n",
    "$$\n",
    "    L_hv=\\lambda v \\ \\Longleftrightarrow \\ \\frac{1}{h^2}Av = \\lambda v\n",
    "$$\n",
    "\n",
    "where $A\\in\\mathbb{R}^{n\\times n}$ is given by\n",
    "\n",
    "$$\n",
    "    A = \\begin{pmatrix}\n",
    "                    2 & -1 & 0 & \\cdots & 0 \\\\\n",
    "                    -1 & 2 & -1 & \\ddots & \\vdots \\\\\n",
    "                    0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "                    \\vdots & \\ddots & -1 & 2 & -1 \\\\\n",
    "                    0 & \\cdots & 0 & -1 & 2\n",
    "                \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and when we write $L_hv$, we mean $v\\in D_{h,0}$ whereas when we write $Av$ we mean $v\\in\\mathbb{R}^n$ is the vector whose $j$th component is defined by the evaluation of the discrete function at the interior grid points $x_j$ for $1\\leq j\\leq n$.\n",
    "\n",
    "The reason we multiply $\\frac{1}{h^2}$ to $A$ is to make the left-hand side of the matrix-vector eigenvalue problem equivalent to $L_hv$ since $L_h$ involves the $\\frac{1}{h^2}$ term in its definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca91cc-27b1-4ce4-8928-d7191812a826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8516dad-7600-4efa-8549-4a046150c171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
