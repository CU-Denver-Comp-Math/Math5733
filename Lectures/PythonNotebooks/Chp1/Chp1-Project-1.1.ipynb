{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1.1: *Convergence of Sequences*\n",
    "\n",
    "The purpose of this project is to develop a ***quick and dirty*** way of investigating the convergence of schemes under the condition that the *exact* solution is available.\n",
    "\n",
    "The general idea is to study how a method converges for a particular type of problem when the solution is known in order to guide us in learning how the scheme handles more delicate cases.\n",
    "\n",
    "We are essentially dealing with what is known as the **method of manufactured solutions** which is a verification approach for code. \n",
    "\n",
    "We will reference the method of manufactured solutions many more times throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Definitions and Notation\n",
    "\n",
    "**Sequence**. Let $X$ be a set. For $m\\in\\mathbb{Z}$, a sequence is an ordered set of terms from $X$ denoted by $z_m, z_{m+1}, z_{m+2}, \\ldots$ (with some terms possibly repeated a finite or infinite amount of times). We often use the more compact notation $(z_n)_{n=m}^\\infty\\subset X$ or $\\{z_n\\}_{n=m}^\\infty\\subset X$ to denote this set. When $m=1$ or it is understood from context what $m$ is, then we often denote the sequence even more simply as $(z_n)$ or $\\{z_n\\}$.\n",
    "\n",
    "**Convergence of Sequences**. Let $(z_n)\\subset\\mathbb{R}$ and $z\\in\\mathbb{R}$. We say that $(z_n)$ converges to $z$ if for every $\\epsilon>0$ there exists an integer $N$ such that for all integers $n\\geq N$, \n",
    "\n",
    "$$ \\large |z_n-z| < \\epsilon.$$\n",
    "\n",
    "- If $\\{z_n\\}$ converges to $z$, then we write either \n",
    "\n",
    "$$\\large \\lim_{n\\to\\infty} z_n=z, \\ \\text{ or more simply } \\ z_n\\to z.$$\n",
    "\n",
    "- In the definition, $N$ can actually be a real number, but $n$ must be an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rate of Convergence**. Let $(z_n)\\subset\\mathbb{R}$ and $z\\in\\mathbb{R}$. We say that $z_n\\to z$ with the rate $\\alpha\\in\\mathbb{R}$ if there exists a constant $c\\in\\mathbb{R}$ (not depending on $n$) such that\n",
    "\n",
    "$$ \\large |z_n-z| \\leq c\\left(\\frac{1}{n}\\right)^\\alpha. $$\n",
    "   \n",
    "- If $\\alpha=1$, we say that the rate of convergence is either first-order or linear.\n",
    "\n",
    "- If $\\alpha=2$, we say that the rate of convergence is either second-order of quadratic.\n",
    "You should get the idea for $\\alpha > 2$.\n",
    "\n",
    "***Superlinear Convergence***. We say that a sequence $(z_n)\\subset\\mathbb{R}$ converges superlinearly towards $z\\in\\mathbb{R}$ if there is a positive sequence of real numbers $\\{c_n\\}$ such that $c_n\\to 0$ and $|z_n-z| \\leq c_n/n$.\n",
    "\n",
    "***The $O$-Notation (\"Big-Oh\" Notation)***. Let $(y_n)$ and $(z_n)$ be two sequences of positive real numbers. If there is a finite constant $c$, not depending on $n$, such that \n",
    "\n",
    "$$\\large y_n \\leq cz_n \\ \\ \\ \\forall \\ n\\geq 1,$$\n",
    "\n",
    "we say that the sequence $(y_n)$ is of order $(z_n)$, and we write, \n",
    "\n",
    "$$\\large y_n=O(z_n).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a): Estimating the rate of convergence for some sequences.\n",
    "\n",
    "1. $z_n = \\sqrt{1/n}$\n",
    "\n",
    "2. $z_n = \\sin(1/n)$\n",
    "\n",
    "3. $z_n = \\sqrt{1/n} \\sin^2(1/n)$\n",
    "\n",
    "4. $z_n = n(e^{(1/n)} - 1 - 1/n)$\n",
    "\n",
    "We first must determine the limit, which is not always obvious. \n",
    "Plots are useful tools.\n",
    "We therefore first make use of the ``numpy`` and ``matplotplib`` libraries to develop some intuition about these sequences.\n",
    "Below, we proceed in the following steps:\n",
    "\n",
    "  i. We import ``numpy`` and ``matplotlib`` (and also enable plotting within this notebook)\n",
    "    \n",
    "  ii. We generate plots of the sequences to help numerically \"guess\" at the limits (assuming they exist). Some of these are obvious, but others are less so.\n",
    "    \n",
    "  iii. Once we determine the appropriate limit $z$ in each case. We note that taking logarithms of both sides of the inequality in the definition of rate of convergence yields\n",
    "    \n",
    "$$\\large \\ln |z_n-z| \\leq \\alpha \\ln (c/n) = \\alpha (\\ln c + \\ln (1/n)). $$\n",
    "   \n",
    "   Thus, we can perform regression using ``polyfit`` within ``numpy`` (see https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html for more info) to fit a line to the logarithm of $|z_n-z|$ and $1/n$ and take the slope of that line to get an estimate of the rate of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step ii. First create parts of the sequences in the next two code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(1,100,100)  # generate an array from 1 to 100 of floats\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.sqrt(1./n)  # Seq. 1\n",
    "z2 = np.sin(1./n)  # Seq. 2\n",
    "z3 = np.sqrt(1./n) * np.sin(1./n)**2  # Seq. 3\n",
    "z4 = n * (np.exp(1./n) - 1 - 1./n)  # Seq. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step ii. Now plot the created sequences and infer the limits from the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(20,5))  # Make figure 1 size 20 wide by 5 height (opposite of how we specify arrays by height vs width)\n",
    "\n",
    "ax1 = fig.add_subplot(1, 4, 1)  # 1x4 array - subplot 1\n",
    "ax2 = fig.add_subplot(1, 4, 2)  # 1x4 array - subplot 2\n",
    "ax3 = fig.add_subplot(1, 4, 3)  # 1x4 array - subplot 3\n",
    "ax4 = fig.add_subplot(1, 4, 4)  # 1x4 array - subplot 4\n",
    "\n",
    "ax1.set_title('$z_n = \\sqrt{1/n}$')\n",
    "ax2.set_title('$z_n = \\sin(1/n) $')\n",
    "ax3.set_title('$z_n = \\sqrt{1/n}\\sin^2(1/n)$')\n",
    "ax4.set_title('$z_n = n(e^{1/n} - 1 - 1/n)$')\n",
    "\n",
    "ax1.set_xlim(1, 100)\n",
    "ax2.set_xlim(1, 100)\n",
    "ax3.set_xlim(1, 100)\n",
    "ax4.set_xlim(1, 100)\n",
    "\n",
    "ax1.scatter(n, z1)\n",
    "ax2.scatter(n, z2)\n",
    "ax3.scatter(n, z3)\n",
    "ax4.scatter(n, z4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step iii. Use ``polyfit`` within ``numpy`` to determine the rate of convergence as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_lim = 0.0  # Taken from previous plots\n",
    "z2_lim = 0.0  # Taken from previous plots\n",
    "z3_lim = 0.0  # Taken from previous plots\n",
    "z4_lim = 0.0  # Taken from previous plots\n",
    "\n",
    "# Remember to always take the absolute value before computing the log of errors since errors can be positive or negative\n",
    "z1_log = np.log(np.abs(z1 - z1_lim))  \n",
    "z2_log = np.log(np.abs(z2 - z2_lim))\n",
    "z3_log = np.log(np.abs(z3 - z3_lim))\n",
    "z4_log = np.log(np.abs(z4 - z4_lim))\n",
    "\n",
    "# If we append a [0] to the end of the following polyfit calls, then we only return the slopes\n",
    "line1_params = np.polyfit(np.log(1./n), z1_log, 1)\n",
    "line2_params = np.polyfit(np.log(1./n), z2_log, 1)\n",
    "line3_params = np.polyfit(np.log(1./n), z3_log, 1)\n",
    "line4_params = np.polyfit(np.log(1./n), z4_log, 1)\n",
    "\n",
    "print(\"[slope, intercept] log-log line fit for z1 = \", line1_params)\n",
    "print(\"[slope, intercept] log-log line fit for z2 = \", line2_params)\n",
    "print(\"[slope, intercept] log-log line fit for z3 = \", line3_params)\n",
    "print(\"[slope, intercept] log-log line fit for z4 = \", line4_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step iii. Create plots to visually confirm the rate of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(2, figsize=(20,20))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)  # 1x4 array - subplot 1\n",
    "ax2 = fig.add_subplot(2, 2, 2)  # 1x4 array - subplot 2\n",
    "ax3 = fig.add_subplot(2, 2, 3)  # 1x4 array - subplot 3\n",
    "ax4 = fig.add_subplot(2, 2, 4)  # 1x4 array - subplot 4\n",
    "\n",
    "axs = [ax1, ax2, ax3, ax4]\n",
    "line_params = [line1_params, line2_params, line3_params, line4_params]\n",
    "zs_log = [z1_log, z2_log, z3_log, z4_log]\n",
    "\n",
    "for i in range(4):\n",
    "    axs[i].plot(np.log(1./n), zs_log[i], label='log of $|z_' + str(i) + '-z_{' + str(i) + ',lim}|$')\n",
    "    \n",
    "    slope_str = \"{:1.2f}\".format(line_params[i][0]) \n",
    "    \n",
    "    axs[i].plot(np.log(1./n), line_params[i][0]*np.log(1./n)+line_params[i][1]+1,\n",
    "               'r', label='slope = ' + slope_str)\n",
    "    \n",
    "    title_str = '$z_' + str(i) + '$ Rate of Conv. ' + slope_str\n",
    "    \n",
    "    axs[i].set_title(title_str)\n",
    "    axs[i].set_xlabel('log of $1/n$')\n",
    "    axs[i].legend(loc='upper left', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Linear or superlinear convergence?\n",
    "\n",
    "1. $\\large z_n = 1/n$\n",
    "\n",
    "2. $\\large z_n = \\frac{1}{n\\log(n)}$\n",
    "\n",
    "3. $\\large z_n = \\frac{e^{1/n}}{n}$\n",
    "\n",
    "From the definition, it is quite clear that only the second sequence converges superlinearly. \n",
    "\n",
    "However, as we will see below in part (c) when we replace $n$ with $h=1/n$, the logarithm can make detecting this quite difficult (try playing with the ``offset`` as mentioned below).\n",
    "Moreover, it is perhaps unclear from the text on how to numerically test for superlinear convergence. \n",
    "Basically, we are looking for an $\\alpha$ that is somewhere between $1$ and $2$. \n",
    "It is a good idea to test this for fairly large values of $n$ to let asymptotics take over. \n",
    "You may only want to test the tail end of a sequence which numerically means testing a segment of the tail that is sufficiently far from the beginning.\n",
    "\n",
    "Below, we can test with various values of ``offset`` to conclude that the second sequence is superlinear in convergence but the first and third are not even though for an ``offset=0`` we may erroneously conclude that the third sequence is superlinearly convergent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 1E3 \n",
    "\n",
    "n = np.linspace(offset+1,offset+100,100)\n",
    "\n",
    "z1 = 1./n\n",
    "z2 = 1./((n+1)*np.log(n+1))  # This sequence must start at n=2 not n=1\n",
    "z3 = np.exp(1./n)/(n)\n",
    "\n",
    "z1_log = np.log(np.abs(z1))\n",
    "z2_log = np.log(np.abs(z2))\n",
    "z3_log = np.log(np.abs(z3))\n",
    "\n",
    "line1_params = np.polyfit(np.log(1./n), z1_log, 1)\n",
    "line2_params = np.polyfit(np.log(1./n), z2_log, 1)\n",
    "line3_params = np.polyfit(np.log(1./n), z3_log, 1)\n",
    "\n",
    "print(\"rate of conv. for z1 = \", line1_params[0])\n",
    "print(\"rate of conv. for z2 = \", line2_params[0])\n",
    "print(\"rate of conv. for z3 = \", line3_params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Redux with $h=1/n$\n",
    "\n",
    "When discretizing PDEs, we often let $h$ denote a spatial discretization parameter in a particular direction. \n",
    "If we use \"regular\" discretizations, then this means we specify a number of points in each direction to define a grid where the spacing of points in a particular direction is uniform, so then $h=1/n$. Thus, as $n\\to\\infty$, $h\\to 0$, so all the definitions are easily changed by making the substitutions of $h$ for $1/n$ or $h\\to 0$ for $n\\to\\infty$.\n",
    "We now estimate the rates of convergence as $h\\to 0$ for\n",
    "\n",
    "1. $z_h = \\sqrt{h}\\sin(h)$\n",
    "\n",
    "2. $z_h = \\sqrt{h}\\cos(h)$\n",
    "\n",
    "3. $z_h = \\sqrt{h} e^h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1./n\n",
    "\n",
    "z1 = np.sqrt(h)*np.sin(h)\n",
    "z2 = np.sqrt(h)*np.cos(h)\n",
    "z3 = np.sqrt(h)*np.exp(h)\n",
    "\n",
    "z1_log = np.log(np.abs(z1))\n",
    "z2_log = np.log(np.abs(z2))\n",
    "z3_log = np.log(np.abs(z3))\n",
    "\n",
    "line1_params = np.polyfit(np.log(1./n), z1_log, 1)\n",
    "line2_params = np.polyfit(np.log(1./n), z2_log, 1)\n",
    "line3_params = np.polyfit(np.log(1./n), z3_log, 1)\n",
    "\n",
    "print(\"rate of conv. for z1 = \", line1_params[0])\n",
    "print(\"rate of conv. for z2 = \", line2_params[0])\n",
    "print(\"rate of conv. for z3 = \", line3_params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d): Some basic analysis\n",
    "\n",
    "When we discuss a smooth function, we basically mean \n",
    "> A smooth function is as smooth as we need it to be. In other words, it has as many derivatives as is required by the analysis.\n",
    "\n",
    "What is shown here are simple finite difference formulas for estimating derivatives of a ***smooth*** function of a single variable. \n",
    "They are all derived via simple Taylor series expansions. \n",
    "\n",
    "1. $\\large \\frac{f(x+h) - f(x)}{h} = f'(x) + O(h)$\n",
    "\n",
    "2. $\\large \\frac{f(x) - f(x-h)}{h} = f'(x) + O(h)$\n",
    "\n",
    "3. $\\large \\frac{f(x+h) - f(x-h)}{h} = f'(x) + O(h^2)$\n",
    "\n",
    "4. $\\large \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} = f''(x) + O(h^2)$\n",
    "\n",
    "I will only derive the first and third ones.\n",
    "\n",
    "#### Derivation of 1.\n",
    "By the Taylor series formula we have\n",
    "\n",
    "$$ \\large\n",
    "\\begin{align}\n",
    " f(x+h) &= f(x) + f'(x)h + f''(x)\\frac{h^2}{2} + O(h^3) \\\\ \\\\\n",
    " \\Rightarrow f(x+h) - f(x) &= f'(x)h + f''(x)\\frac{h^2}{2} + O(h^3) \\\\ \\\\ \n",
    " \\Rightarrow \\frac{f(x+h)-f(x)}{h} &= f'(x) + + f''(x)\\frac{h}{2} + \\frac{O(h^3)}{h} \\\\ \\\\\n",
    " \\Rightarrow \\frac{f(x+h) - f(x)}{h} &= f'(x) + O(h) + O(h^2) \\\\ \\\\\n",
    " \\Rightarrow \\frac{f(x+h) - f(x)}{h} &= f'(x) + O(h) \\ \\Box\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Derivation of 3.\n",
    "By the Taylor series formula we have both\n",
    "\n",
    "$$\n",
    " \\large   f(x+h) = f(x) + f'(x)h + f''(x)\\frac{h^2}{2} + f'''(x)\\frac{h^3}{6}+O(h^4),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    " \\large   f(x-h) = f(x) - f'(x)h + f''(x)\\frac{h^2}{2} - f'''(x)\\frac{h^3}{6}+O(h^4).\n",
    "$$\n",
    "\n",
    "Then, we compute $f(x+h)-f(x-h)$ from which it follows that\n",
    "\n",
    "$$\n",
    " \\large   f(x+h) - f(x-h) = 2f'(x)h + f'''(x)\\frac{h^3}{3} + h.o.t.\n",
    "$$\n",
    "\n",
    "and dividing by $2h$ to both sides gives the result. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e): Now we're getting to the point. \n",
    "\n",
    "You may have wondered why all the sequences we have looked at so far have zero as the limit.\n",
    "Well, we are almost exclusively pre-occupied with studying the convergence of a method, which means that the ***error should converge to zero as we refine the method***.\n",
    "\n",
    "Thus, when we test a method, we often will **manufacture a solution** and use some norm to compute the error in a numerical approximation to the manufatured solution. \n",
    "If the method is worth using, then the error should converge to zero, and given a robust set of manufactured solutions, we may even be able to numerically estimate the rate of convergence. \n",
    "\n",
    "The numerics of this part of the problem are not really important. \n",
    "What is important is the idea and how it is explaining an approach based on using iterative refinements to estimate the rate of convergence. \n",
    "Previously, we used regression on the logarithm of multiple terms in a sequence that were cheap to evaluate.\n",
    "When numerically solving a PDE, we often can only refine a mesh a few times due to computational constraints.\n",
    "Then, we would use an equation like (1.63) in the text to estimate the rate of convergence for some particular choices of $h$ values like those given.\n",
    "\n",
    "Deriving (1.63) is quite easy, simply take the log of (1.62) and then compute the slope of the line through the two points for $h_1$ and $h_2$ (i.e., take the \"rise\" over the \"run\"). \n",
    "Then, use the rules of logarithms to rewrite differences in logs as the log of a ratio. \n",
    "\n",
    "We can easily test how using (1.63) compares to our regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_init = 1./100\n",
    "h = h_init * np.array([1, 1./2, 1./2**2, 1./2**3])\n",
    "print(h)\n",
    "\n",
    "z1 = np.sqrt(h)*np.sin(h)\n",
    "\n",
    "alpha = np.log(z1[1:-1]/z1[0:-2])/np.log(h[1:-1]/h[0:-2])\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f): Be careful out there kiddos.\n",
    "\n",
    "What happens to $z_h = |h\\log(h)|$ as $h\\to 0$ (and it should be clear we really mean $h\\downarrow 0$)? Does the limit even exist?\n",
    "We rewrite $z_h$ and observe that the limit is a so-called indeterminate form\n",
    "\n",
    "$$\n",
    " \\large   z_h = \\left|\\frac{\\log h}{1/h}\\right| \\to \\frac{\\infty}{\\infty} \\ \\ \\text{ as } h\\to 0.\n",
    "$$\n",
    "\n",
    "We then recall a certain rule about hospitals from calculus to get\n",
    "\n",
    "$$\n",
    " \\large   \\lim_{h\\to 0} z_h = \\lim_{h\\to 0} \\left|\\frac{1/h}{-1/h^2}\\right| = \\lim_{h\\to 0} |h| = 0.\n",
    "$$\n",
    "\n",
    "Let's play with this numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_init = 1./100\n",
    "h = h_init * np.array([1, 1./2, 1./2**2, 1./2**3, 1./2**4, \n",
    "                       1./2**5, 1./2**10, 1./2**11, 1./2**12,\n",
    "                       1./2**20, 1./2**21, 1./2**40, 1./2**41])\n",
    "\n",
    "z_lame = np.abs(h*np.log(h))\n",
    "print(z_lame)\n",
    "\n",
    "alpha = np.log(z_lame[1:-1]/z_lame[0:-2])/np.log(h[1:-1]/h[0:-2])\n",
    "print('-'*50)\n",
    "print('Estimated ROC with successive differences: \\n', alpha)\n",
    "\n",
    "print('-'*50)\n",
    "print('Using regression to estimate ROC: ', np.polyfit(np.log(h), np.log(z_lame), 1)[0], '\\n')\n",
    "\n",
    "fig = plt.figure(4, figsize=(5,5))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(np.log(h[1:-1]), alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um, what do we see? It looks like the ``alpha`` array is monotonically increasing as $h$ decreases to 0. It is not clear if there is numerically a good representative value of the rate of convergence $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (g): Using Fourier series to estimate irrational numbers.\n",
    "\n",
    "I am skipping most of this, but students should give all of these a try. \n",
    "I will do part (i).\n",
    "\n",
    "(i) $\\large z=\\pi/4$ and $\\large z_n=\\sum_{j=0}^n \\frac{(-1)^j}{2j+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "n = np.linspace(0,99,N)\n",
    "z_n = np.zeros(N)\n",
    "z = np.pi/4\n",
    "for k in n.astype('int'):\n",
    "    for j in range(0,k):\n",
    "        z_n[k] += (-1.)**j/(2*j+1)\n",
    "\n",
    "print(z)\n",
    "print(z_n)\n",
    "\n",
    "e_n = np.abs(z-z_n)\n",
    "\n",
    "alpha = np.log(e_n[1:-1]/e_n[0:-2])/np.log(n[0:-2]/n[1:-1])  # Note the switch in the denominator b/c h=1/n\n",
    "print(alpha)\n",
    "\n",
    "fig = plt.figure(4, figsize=(10,10))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(np.log(1./n), np.log(e_n), label='Log $|z_n-z|$ part (i)')\n",
    "slope_str = \"{:1.2f}\".format(alpha[-1])\n",
    "axes.plot(np.log(1./n), alpha[-1]*np.log(1./n),\n",
    "           'r', label='slope = ' + slope_str)\n",
    "axes.set_xlabel('log of $1/n$')\n",
    "axes.legend(loc='upper left', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
