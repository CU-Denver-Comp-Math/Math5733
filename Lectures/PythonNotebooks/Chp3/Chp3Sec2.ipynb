{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc79548-e757-4dca-947b-8adf38080bd8",
   "metadata": {},
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1265a-ccb8-455f-b3eb-97f224e7dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fc71d-e374-4021-80e1-90ca4505c98a",
   "metadata": {},
   "source": [
    "## Section 3.2: A ([Dream Theater](https://dreamtheater.net/) inspired) Crash Course in Fourier Series\n",
    "---\n",
    "\n",
    "This section primarily focuses on the summary of definitions and theory for general [Fourier series](https://en.wikipedia.org/wiki/Fourier_series)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd1506-cb56-41ce-a778-e56d1d19795a",
   "metadata": {},
   "source": [
    "### Section 3.2.1: (Some) Scenes from a Memory\n",
    "---\n",
    "\n",
    "**A Strange Déjà Vu.** When you create a visual representation of [vectors](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)) in $\\mathbb{R}^3$ in your mind, what do you see? Do you see 3-D arrows? The arrows point to something, but what is it that they point at?\n",
    "\n",
    "![Vectors from Gram-Schmidt process](https://upload.wikimedia.org/wikipedia/commons/e/ee/Gram-Schmidt_orthonormalization_process.gif)\n",
    "\n",
    "Mathematically we represent a vector in $v\\in\\mathbb{R}^3$ as an array of numbers such as\n",
    "\n",
    "$$\n",
    "    v = \\begin{pmatrix}\n",
    "            v_1 \\\\\n",
    "            v_2 \\\\\n",
    "            v_3\n",
    "        \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Here, $v_1, v_2$ and $v_3$ are real numbers representing the components of $v$. Once we become comfortable (or at least familiar enough) with the mathematical rules governing how we interact with vectors, we often think nothing more of this. But, there is a deeper story to be told.\n",
    "\n",
    "Let $e^{(1)}, e^{(2)}$ and $e^{(3)}$ denote the *standard basis vectors* in $\\mathbb{R}^3$ defined by \n",
    "\n",
    "$$\n",
    "    e^{(1)} = \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            0 \\\\\n",
    "            0\n",
    "        \\end{pmatrix}, \\ \n",
    "    e^{(2)} = \\begin{pmatrix}\n",
    "            0 \\\\\n",
    "            1 \\\\\n",
    "            0\n",
    "        \\end{pmatrix}, \\ \\text{ and } \\\n",
    "    e^{(3)} = \\begin{pmatrix}\n",
    "            0 \\\\\n",
    "            0 \\\\\n",
    "            1\n",
    "        \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a12b7-961b-4175-b724-2e79e495803a",
   "metadata": {},
   "source": [
    "**Through My Words.** Each of these basis vectors represents a *fundamental direction* in $\\mathbb{R}^3$. It is common to refer to $e^{(1)}$ as *pointing* in the $x$-direction, $e^{(2)}$ as *pointing* in the $y$-direction, and $e^{(3)}$ as *pointing* in the $z$-direction. This is common because it is intuitive. It matches our typical mathematical description of 3-D space in terms of orthorgonal $x$-, $y$-, and $z$-axes and our visual conceptualization of vectors as arrows that point.\n",
    "\n",
    "We also see that if $\\langle \\cdot, \\cdot \\rangle$ denotes the typical inner product structure on $\\mathbb{R}^3$, then $v_1 = \\langle v, e^{(1)} \\rangle$, $v_2 = \\langle v, e^{(2)} \\rangle$, and $v_3 = \\langle v, e^{(3)} \\rangle$. Moreover, we can rewrite $v\\in\\mathbb{R}^3$ as\n",
    "\n",
    "$$\n",
    "    v = v_1e^{(1)} + v_2e^{(2)} + v_3e^{(3)}.\n",
    "$$\n",
    "\n",
    "This means that although $v$ is a single vector, it can be decomposed as the sum of three vectors. If we think of each vector as describing a way of traveling through the space in which the vector exists, then we may think of traveling $v_1$ units in the $x$-direction, then \"turning 90 degrees\" in the $xy$-plane, we travel $v_2$ units in the $y$-direction, and finally, \"turning another 90 degrees\" (this time in the $yz$-plane), we travel a final $v_3$ units to arrive at the desired location at the tip of the arrow of $v$. \n",
    "\n",
    "W can easily generalize the above description to vectors in $n$-dimensional Euclidean space, $\\mathbb{R}^n$ for an arbitrarily large positive integer $n$. We do this by considering the standard basis vectors of the form $e^{(i)}\\in\\mathbb{R}^n$,  for $1\\leq i\\leq n$, that have a $1$ in the $i$th coordinate and a $0$ everywhere else.\n",
    "\n",
    "None of this, of course, is profound. We are often exposed to this idea of vectors in a first-semester linear algebra course (or perhaps before that a multivariable calculus course). But, this hints at deeper ideas that, while also introduced and considered (if ever so briefly) in a first-semester linear algebra course, are also perhaps not fully appreciated at the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e674c-a924-48f8-8560-02fc4b0a0b3f",
   "metadata": {},
   "source": [
    "**Fatal Tragedy (of standard basis vectors).** We are not always interested in using the standard basis vectors. These represent, after all, a standard representation of the Euclidean space (of whatever dimension we are considering). However, there are plenty of applications where we wish to, for lack of a better term, orient ourselves in the space in a manner different from the standard approach. For instance, when the space is, for lack of a better term, warped by a mapping, then representing vectors as a summation of the standard basis vectors can unwittingly hide useful insights into how these vectors are situated in the space with respect to the mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5e160-e2e0-4991-850c-04f09562cf02",
   "metadata": {},
   "source": [
    "**Beyond This Life (of standard basis vectors).** Consider, for instance, the standard linear mapping of a space into itself with a symmetric positive definite matrix $A\\in\\mathbb{R}^{n\\times n}$. Assume, for the sake of simplicity, that $A$ has $n$ distinct positive eigenvalues denoted by $\\lambda_1, \\ldots, \\lambda_n$, and $v^{(1)}, \\ldots, v^{(n)}$ are the associated eigenvectors. Then, $Av^{(i)} = \\lambda_i v^{(i)}$ for each $1\\leq i\\leq n$. We also have that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\lambda_i \\langle  v^{(i)},  v^{(j)} \\rangle &= \\langle  \\lambda_i v^{(i)},  v^{(j)} \\rangle \\\\\n",
    "                                                         \\\\\n",
    "                                                         &= \\langle  Av^{(i)},  v^{(j)} \\rangle \\\\\n",
    "                                                         \\\\\n",
    "                                                         &= \\langle  v^{(i)}, A^\\top v^{(j)} \\rangle \\\\\n",
    "                                                         \\\\\n",
    "                                                         &= \\langle  v^{(i)}, A v^{(j)} \\rangle \\\\\n",
    "                                                         \\\\\n",
    "                                                         &= \\langle  v^{(i)}, \\lambda_j v^{(j)} \\rangle \\\\\n",
    "                                                         \\\\\n",
    "                                                         &= \\lambda_j \\langle  v^{(i)}, v^{(j)} \\rangle.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663260c-42e0-4e64-aae0-d3829d2d628d",
   "metadata": {},
   "source": [
    "**Through Her Eyes (an eigenbasis).** This means that $\\lambda_i  \\langle  v^{(i)},  v^{(j)} \\rangle = \\lambda_j \\langle  v^{(i)}, v^{(j)} \\rangle$, or, to put it another way, $\\lambda_i \\alpha = \\lambda_j \\alpha$ where $\\alpha = \\langle  v^{(i)}, v^{(j)} \\rangle$. Since $\\lambda_i\\neq\\lambda_j$ whenever $i\\neq j$ by assumption, this can only be true if $\\alpha=0$. This of course means that the eigenvectors are *orthogonal* to each other. We are free to choose non-trivial scalar multiples of each eigenvector to describe the eigenspace associated with each eigenvector, and we might as well choose them so that $\\langle v^{(i)}, v^{(i)} \\rangle = 1$ for $1\\leq i\\leq n$. Such a choice then defines an *orthonormal basis* for $\\mathbb{R}^n$. \n",
    "\n",
    "This is a very useful basis because if we consider any $v\\in\\mathbb{R}^n$, we immediately have that\n",
    "\n",
    "$$\n",
    "    v = \\tilde{v}_1 v^{(1)} + \\tilde{v}_2 v^{(2)} + \\cdots \\tilde{v}_n v^{(n)}, \n",
    "$$\n",
    "\n",
    "where, by the assumed orthonormality of the eigenvectors, we have that\n",
    "\n",
    "$$\n",
    "    \\tilde{v}_1 = \\langle v, v^{(1)} \\rangle, \\ \\tilde{v}_2 = \\langle v, v^{(2)} \\rangle, \\ \\ldots \\, \\tilde{v}_n = \\langle v, v^{(n)}\\rangle.\n",
    "$$\n",
    "\n",
    "This decomposition of $v$ in terms of the orthonormal basis described by the eigenvectors of $A$ is incredibly useful because it tells us immediately what $Av$ is *without ever having to compute the matrix-vector product*. Instead, we can simply write\n",
    "\n",
    "$$\n",
    "    Av = \\lambda_1 \\tilde{v}_1 v^{(1)} + \\lambda_2\\tilde{v}_2 v^{(2)} + \\cdots \\lambda_n\\tilde{v}_n v^{(n)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c712f8-f965-4d19-8f72-ce77c3465a79",
   "metadata": {},
   "source": [
    "**(Bringing it all) Home.** What the equation above means is that it is more useful to \"orient our perspective\" in such a way in the $\\mathbb{R}^n$ space so that we think of an arbitrary vector $v$ as the summation of eigenvectors, $v^{(i)}$ for $1\\leq i\\leq n$, that are themselves arrows pointing in directions *preferred by $A$* in the sense that the direction of these arrows remains unchanged when transformed by $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb6da8-550e-4ad3-b22d-4883b475a93a",
   "metadata": {},
   "source": [
    "### Section 3.2.2: (Some Additional) Scenes from a Memory\n",
    "---\n",
    "\n",
    "**The Dance of Eternity (and an infinite-dimensional vector space).** The basic concept behind a Fourier series is that the trigonmetric family of functions defined by \n",
    "\n",
    "$$\n",
    "    \\left\\{ \\frac{1}{2}, \\ \\cos(k\\pi x), \\ \\sin(k\\pi x), \\ k=1, 2, \\ldots \\right\\}\n",
    "$$\n",
    "\n",
    "forms an [orthonormal basis](https://en.wikipedia.org/wiki/Orthonormal_basis) for functions in [$L^2([-1,1])$](https://en.wikipedia.org/wiki/Square-integrable_function). In other words, if $f$ is a square integrable function on $[-1, 1]$, then we can write $f(x)$ as a (possibly infinite) linear combination of functions taken from this basis. Moreover, since an $L^2$ space is an inner product space (defined by the integration of the product of two $L^2$ functions) and the basis functions are in fact an orthonormal basis, then the coefficients used in the linear combination are easily determined by integrating $f$ against each of these basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd2481-02f3-44e7-b8ac-6c455e7f01e1",
   "metadata": {},
   "source": [
    "**One Last Time.** It is worth emphasizing, just one last time, how eigenvectors are useful vectors for forming a basis. We have seen how sine functions show up as eigenfunctions associated with certain two-point BVPs with homogeneous Dirichlet BCs. We would have seen cosine functions as eigenfunctions if the BCs were of Neumann type. Committing this to memory is a good idea.\n",
    "\n",
    "Another useful note is that if $f(y)$ is a real-valued function defined for $y\\in[0,P]$, then redefining the domain through a change of variables $y(x)=\\frac{P(x+1)}{2}$ allows us to define \"$f(x)$\" (meaning $f(y(x))$) as a function on $[-1,1]$, which we find convenient here.\n",
    "\n",
    "We therefore assume for simplicity that the functions considered in this notebook are defined on $[-1,1]$ unless otherwise specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65305562-4740-460b-bd86-f442ddc02768",
   "metadata": {},
   "source": [
    "**The Spirit Carries On.** For such a function $f$, the full Fourier series is given by\n",
    "\n",
    "$$\n",
    "    f(x) \"=\" \\frac{a_0}{2} + \\sum_{k=1}^\\infty \\left(a_k\\cos(k\\pi x) + b_k\\sin(k\\pi x)\\right), \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    a_k := \\int_{-1}^1 f(x)\\cos (k\\pi x)\\, dx, \\ k=0, 1, 2, \\ldots, \n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    b_k := \\int_{-1}^1 f(x)\\sin (k\\pi x)\\, dx, \\ k=1, 2, 3, \\ldots.\n",
    "$$\n",
    "\n",
    "This truly is in the spirit of what we have seen before for finite-dimensional vectors written in terms of a convenient basis (such as an orthonormal basis of eigenvectors). If we let $\\langle \\cdot, \\cdot \\rangle$ denote the $L^2$ inner product on $[-1,1]$, then we see that each of the coefficients of the Fourier series is simply given as $\\langle f, v^{(i)}\\rangle$ where $v^{(i)}$ is one of the functions from the trigonometric family given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b42de-108d-4af0-9c11-20900afa11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1461b-1ca0-4049-bb48-d03691ee1c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_a_k(f, k):\n",
    "    integrand = lambda x: f(x)*np.cos(k*np.pi*x)\n",
    "    a_k = quad(integrand, -1, 1)[0]  # Use quadrature to integrate\n",
    "    if np.abs(a_k) < 1e-7:  # Filter out \"numerically zero\" integrals\n",
    "        return 0\n",
    "    else:\n",
    "        return a_k  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46939044-324f-49a2-82ae-a10459673537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_b_k(f, k):\n",
    "    integrand = lambda x: f(x)*np.sin(k*np.pi*x)\n",
    "    b_k = quad(integrand, -1, 1)[0]  # Use quadrature to integrate\n",
    "    if np.abs(b_k) < 1e-7:  # Filter out \"numerically zero\" integrals\n",
    "        return 0\n",
    "    else:\n",
    "        return b_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b791be8-600a-4188-9124-9609f1d1a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_Fourier_expansion_coeffs(f, k):\n",
    "    \n",
    "    # Compute Fourier coeffs up to and including order k\n",
    "    a_k = np.zeros(k+1)\n",
    "    b_k = np.zeros(k)\n",
    "    \n",
    "    a_k[0] = compute_a_k(f, 0)\n",
    "    \n",
    "    for i in range(1, k+1):\n",
    "        a_k[i] = compute_a_k(f, i)\n",
    "        b_k[i-1] = compute_b_k(f, i)\n",
    "        \n",
    "    return a_k, b_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b787d9-4d99-42c7-a096-d1044237a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Fourier_expansion(a_k, b_k, x):\n",
    "    \n",
    "    partial_sum = a_k[0]/2\n",
    "    for i in range(1, len(b_k)+1):\n",
    "        partial_sum += a_k[i]*np.cos(i*np.pi*x) + b_k[i-1]*np.sin(i*np.pi*x)\n",
    "    \n",
    "    return partial_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfd909-5a08-45cb-bcd9-82ad1cadea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 1\n",
    "\n",
    "a_k, b_k = construct_Fourier_expansion_coeffs(f, 10)\n",
    "\n",
    "print(a_k)\n",
    "print(b_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba36c3-5d37-46a5-b7a4-7f77a8f9e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_Fourier = lambda x: evaluate_Fourier_expansion(a_k, b_k, x)\n",
    "\n",
    "x = np.linspace(-1, 1, 100)\n",
    "\n",
    "print(f_Fourier(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd38670-a367-462d-aec9-87b204b8d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x\n",
    "\n",
    "a_k, b_k = construct_Fourier_expansion_coeffs(f, 10)\n",
    "\n",
    "print(a_k)\n",
    "print(b_k)\n",
    "\n",
    "f_Fourier = lambda x: evaluate_Fourier_expansion(a_k, b_k, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb40aa6-59f9-475f-b9e8-8b017cba61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(0)\n",
    "\n",
    "x_domain = np.linspace(-1, 1, 100)\n",
    "x_extension = np.linspace(-3, 3, 300)\n",
    "\n",
    "plt.plot(x_extension, f_Fourier(x_extension), 'r', label='Fourier approx. on $\\mathbb{R}$')\n",
    "plt.plot(x_domain, f(x_domain), 'b', lw=2, label='f(x) on [-1,1]')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a342e2-57fd-45da-af88-27c51d5ea403",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.heaviside(x+0.5, 1) - np.heaviside(x-0.5, 1)\n",
    "\n",
    "a_k, b_k = construct_Fourier_expansion_coeffs(f, 10)\n",
    "\n",
    "print(a_k)\n",
    "print(b_k)\n",
    "\n",
    "f_Fourier = lambda x: evaluate_Fourier_expansion(a_k, b_k, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19012c1-d297-4d7f-971a-3cd3930bb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(1)\n",
    "\n",
    "x_domain = np.linspace(-1, 1, 100)\n",
    "x_extension = np.linspace(-3, 3, 300)\n",
    "\n",
    "plt.plot(x_extension, f_Fourier(x_extension), 'r', label='Fourier approx. on $\\mathbb{R}$')\n",
    "plt.plot(x_domain, f(x_domain), 'b', lw=2, label='f(x) on [-1,1]')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b870df-3167-4215-875f-d78e98406e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3-2*x**2\n",
    "\n",
    "a_k, b_k = construct_Fourier_expansion_coeffs(f, 10)\n",
    "\n",
    "print(a_k)\n",
    "print(b_k)\n",
    "\n",
    "f_Fourier = lambda x: evaluate_Fourier_expansion(a_k, b_k, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f5034-a0dc-4a63-bc27-a2dfd2a778ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(2)\n",
    "\n",
    "x_domain = np.linspace(-1, 1, 100)\n",
    "x_extension = np.linspace(-3, 3, 300)\n",
    "\n",
    "plt.plot(x_extension, f_Fourier(x_extension), 'r', label='Fourier approx. on $\\mathbb{R}$')\n",
    "plt.plot(x_domain, f(x_domain), 'b', lw=2, label='f(x) on [-1,1]')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf3f00-b77d-41d5-8136-34696c1a41c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "![](https://i.gifer.com/XlO9.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f06048-34cb-4b94-bde0-f7e3142c831a",
   "metadata": {},
   "source": [
    "You may wonder why the Fourier series above is given in terms of cosines and sines when [Section 3.1](Chp3Sec1.ipynb) involved Fourier series expansions only in terms of sines. The homogeneous Dirichlet boundary conditions of the heat equation are what excluded the cosine terms. Consequently, this is why the sine series expansion of the constant function $f(x)=1$ was so complicated in the previous section. If we had involved different boundary conditions such as homogeneous Neumann, then we would have seen cosine terms instead of sine terms resulting in a rather trivial expansion of $f(x)=1$ since $a_0=2$ and all other coefficients are zero in the expansion of this function (i.e., the \"general\" or \"full\" Fourier series expansion of $f(x)=1$ is simply $f(x)=1$ as shown in code above). In [Section 3.3](Chp3Sec3.ipynb), we return to the study of the heat equation with different boundary conditions to explore this further. For now, we focus more on the analysis of general Fourier series expansions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d966991-0245-4fc0-8af6-00223a43f23f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "![](https://media4.giphy.com/media/GfaZNzU42Snz6dlGhN/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5247-6ef8-46e4-94ee-22a42b001930",
   "metadata": {},
   "source": [
    "**Finally Free (to ask the important question).** The most important question we need to address is\n",
    "\n",
    "> What is meant by the quotes around the equality, \"$=$\", connecting $f(x)$ and the infinite sum?\n",
    "\n",
    "After all, it appears in the examples above that there are some points of $f(x)$ where the Fourier series is approximating $f(x)$ quite well while there are other points where the Fourier series (for some functions) exhibits some serious oscillations. How do we make sense of this? How do we address convergence of the partial sums?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98d9d1-bb25-480b-98ed-0d934804a2a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "![](https://media4.giphy.com/media/d1E1YlkOTe4IfdNC/200.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574bec2-e6fb-49a0-8490-cb6f3d678576",
   "metadata": {},
   "source": [
    "We first answer this by describing the specific manner in which the sequence of partial sums converges to $f(x)$ or another value related to the \"nearby behavior\" of $f(x)$ at a specific fixed $x$ value in the interval $[-1,1]$. In other words, we address the pointwise convergence of the Fourier series. This requires us to consider properties of $f(x)$ at particular $x$ values. To that end, we formally define what it means for $f(x)$ to be piecewise-continuous and piecewise-continouusly differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaccbfc-d35c-4845-b2dc-b952e19c82f9",
   "metadata": {},
   "source": [
    "### Section 3.2.3: (A Vision) Decay of Fourier Coefficients\n",
    "---\n",
    "\n",
    "Here, I follow the general presentation of Section 4.4 of [Partial Differential Equations](http://www.math.toronto.edu/ivrii/PDE-textbook/PDE-textbook.pdf) by Victor Ivrii.\n",
    "\n",
    "The following definition sets up the pointwise properties of $f(x)$ that we will use to consider the manner in which the Fourier series converges to $f(x)$ at various points $x\\in[-1,1]$.\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Definition 3.2.1: Piecewise-Continuity and Piecewise-Differentiability\n",
    "\n",
    "(a) $f(x)$ is ***piecewise-continuous*** on $[-1,1]$ if it is continuous except at a finite set of points and at those points of discontinuity, the one-sided limits exist.\n",
    "\n",
    "(b) $f(x)$ is ***piecewise-continuously differentiable*** if it is continuously differentiable except at a finite set of points and at those points, the one-sided limits of the derivative exist. \n",
    "\n",
    "---\n",
    "<br><br>\n",
    "\n",
    "A result from a typical second-semester calculus course is that if a series has a finite sum, then the terms in the series must converge to zero. Of course, the converse is not true since the decay of terms to zero may be so slow that the series still diverges (recall the standard example of a harmonic series $\\sum_{k=1}^\\infty 1/k$ that diverges to infinity). At any rate, with this in mind, it should not come as a surprise that we have the following useful result regarding the decay of Fourier coefficients for piecewise-continuous functions.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Lemma 3.2.1: Decay of Fourier Coefficients (Bridges in the Sky)\n",
    "\n",
    "If $f(x)$ is piecewise-continuous on $[-1,1]$, then $a_k\\to 0$ and $b_k\\to 0$ as $k\\to\\infty$. More generally, $\\int_{-1}^1 f(x)\\cos (\\omega x)\\, dx \\to 0$ and $\\int_{-1}^1 f(x)\\sin (\\omega  x)\\, dx \\to 0$ as $\\omega\\to\\infty$.\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "***Idea of proof of Lemma 3.2.1.***\n",
    "\n",
    "1. First show that if $f(x)$ is continuously differentiable on $[-1,1]$, then integration by parts implies that $a_k=\\mathcal{O}(1/k)$ and $b_k=\\mathcal{O}(1/k)$.\n",
    "\n",
    "2. Now assume that $f(x)$ is only continuous on $[-1,1]$. A useful result from analysis is that $f(x)$ can be approximately arbitrarily accurately in the $\\sup$-norm metric by a continuously differentiable function. Use this to show that the difference between the Fourier coefficients for $f$ and its more smooth approximation can be made arbitrarily small and that by step 1 this implies $a_k\\to 0$ and $b_k\\to 0$.\n",
    "\n",
    "3. If $f(x)$ is piecewise-continuous, then break up the integral over $[-1,1]$ into the sum of integrals over the intervals on which $f(x)$ is continuous. Now use what was shown in step 2. \n",
    "\n",
    "4. Recognize that all the above arguments also hold with $\\omega\\to\\infty$ not necessarily being integers. $\\Box$\n",
    "\n",
    "---\n",
    "<br><br>\n",
    "\n",
    "**Remark:**\n",
    "\n",
    "- It is straightforward to see that the Lemma also gives $\\int_{-1}^1 f(x)\\cos ((\\omega -\\alpha)x + \\phi)\\, dx \\to 0$ and $\\int_{-1}^1 f(x)\\sin ((\\omega - \\alpha)x + \\phi)\\, dx \\to 0$ as $\\omega\\to\\infty$ for some fixed $\\alpha,\\phi\\in\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648bdb8-b831-4b28-86f9-9989ef6d0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3-2*x**2\n",
    "\n",
    "a_k, b_k = construct_Fourier_expansion_coeffs(f, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46432f76-92d7-4fd1-87a8-27a270f69684",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(3)\n",
    "\n",
    "plt.loglog(np.linspace(0, 100, 101), np.abs(a_k), 'b:', label='$a_k$')\n",
    "\n",
    "plt.loglog(np.linspace(1, 100, 100), np.abs(b_k), 'r-.', label='$b_k$')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3652e2-068c-4dad-8369-4a37511f32e9",
   "metadata": {},
   "source": [
    "### Section 3.2.4: Convergence of Fourier Series (Part I: The X Aspect)\n",
    "---\n",
    "\n",
    "Let $S_N(x)$ denote the partial Fourier sum for a piecewise-continuous $f(x)$, i.e., \n",
    "\n",
    "$$\n",
    "    S_N(x) := \\frac{a_0}{2} + \\sum_{k=1}^N \\left(a_k\\cos(k\\pi x) + b_k\\sin(k\\pi x)\\right).\n",
    "$$\n",
    "\n",
    "Substituting the definitions of $a_k$ and $b_k$ into $S_N(x)$ and using the linearity of the integral operator, we have that\n",
    "\n",
    "$$\n",
    "    S_N(x) := \\int_{-1}^1 K_N(x,y) f(y)\\, dy, \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "    K_N(x,y) := \\frac{1}{2} + \\sum_{k=1}^N \\left[ \\cos(k\\pi y)\\cos(k\\pi x) + \\sin(k\\pi y)\\sin(k\\pi x)\\right].\n",
    "$$\n",
    "\n",
    "Note how $K_N(x,y)$ serves a similar purpose as that of the Green's function, $G(x,y)$, that we encountered in [Section 2.1](../Chp2/Chp2Sec1.ipynb). Specifically, $K_N(x,y)$ and $G(x,y)$ are types of [kernel functions for an integral transform](https://en.wikipedia.org/wiki/Integral_transform) that turns some \"data\" (for lack of a better term) defined by $f(x)$ into a new function that we are interested in studying ($S_N(x)$ here and $u(x)$ in Section 2.1).\n",
    "\n",
    "<mark>**The game is to figure out how to use Lemma 3.2.1 to prove that $S_N(x)$ converges to $f(x)$ at certain values of $x$ and is otherwise related to the average value of $f(x)$ at nearby points of discontinuity.**</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5802cff-bc6a-4249-9b69-ed063cb358e9",
   "metadata": {},
   "source": [
    "**Six Degrees of Inner Turbulence.** We now play a game, and oh what a game it is, in how to use trigonometric identities/techniques to rewrite $K_N$ without the summation notation. To this end, we first use a [sum of angles formula for cosine](https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Angle_sum_and_difference_identities) to get\n",
    "\n",
    "$$\n",
    "    K_N(x,y) = \\frac{1}{2} + \\sum_{k=1}^N \\cos(k\\pi(y-x)).\n",
    "$$\n",
    "\n",
    "While this simplifies $K_N$, we are still left with the summation notation. \n",
    "We can turn this summation into a telescoping sum.\n",
    "\n",
    "To do this, we first observe that manipulating a sum of angles formula for sine twice, we have\n",
    "\n",
    "$$\n",
    "    \\sin(\\alpha)\\cos(\\beta) = \\sin(\\alpha + \\beta) - \\cos(\\alpha)\\sin(\\beta)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "    \\sin(\\alpha)\\cos(\\beta) = \\sin(\\alpha - \\beta) + \\cos(\\alpha)\\sin(\\beta), \n",
    "$$\n",
    "\n",
    "so adding these together equations together and using the \"oddness\" of sine gives\n",
    "\n",
    "$$\n",
    "    \\sin(\\alpha)\\cos(\\beta) = \\frac{1}{2} \\left[\\sin(\\alpha + \\beta) - \\sin(\\beta - \\alpha)\\right].\n",
    "$$\n",
    "\n",
    "Choosing $\\beta = k\\pi(y-x)$ and $\\alpha=\\pi(y-x)/2$, we have\n",
    "\n",
    "$$\n",
    "    \\sin\\left(\\frac{\\pi(y-x)}{2}\\right)\\cos(k\\pi(y-x)) = \\frac{1}{2} \\left[\\sin\\left(\\left(k+\\frac{1}{2}\\right)\\pi(y-x)\\right) - \\sin\\left(\\left(k-\\frac{1}{2}\\right)\\pi(y-x)\\right)\\right].\n",
    "$$\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\sin\\left(\\frac{\\pi(y-x)}{2}\\right)K_N(x) &= \\frac{1}{2}\\sin\\left(\\frac{\\pi(y-x)}{2}\\right) + \\underbrace{\\frac{1}{2}\\sum_{k=1}^N \\left[\\sin\\left(\\left(k+\\frac{1}{2}\\right)\\pi(y-x)\\right) - \\sin\\left(\\left(k-\\frac{1}{2}\\right)\\pi(y-x)\\right)\\right]}_{\\text{telescoping sum}} \\\\\n",
    "                        \\\\\n",
    "                        &= \\frac{1}{2}\\sin\\left(\\frac{\\pi(y-x)}{2}\\right) + \\frac{1}{2}\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right) - \\frac{1}{2}\\sin\\left(\\frac{\\pi(y-x)}{2}\\right) \\\\\n",
    "                        \\\\\n",
    "                        &=  \\frac{1}{2}\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, we get\n",
    "\n",
    "$$\n",
    "    K_N(x) = \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)},\n",
    "$$\n",
    "\n",
    "which implies that\n",
    "\n",
    "$$\n",
    "    S_N(x) = \\int_{-1}^1  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} f(y)\\, dy \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b690f-74ab-4c4d-90f1-c9973678bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_N(y, N, x):\n",
    "    return np.sin((N+0.5)*np.pi*(y-x)) / ( 2*np.sin(np.pi*(y-x)/2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a43585-90fc-4e0b-be16-d3d5e0a8f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think of the cost of this approach vs. computing the N coefficients\n",
    "\n",
    "def S_N(f, N, x):\n",
    "    v = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        integrand = lambda y, N, x: K_N(y, N, x)*f(y)\n",
    "        v[i] = quad(integrand, -1, 1, args=(N, x[i]))[0]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6411a-24d3-4931-ba68-75af830cfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.heaviside(x+0.5, 1) - np.heaviside(x-0.5, 1)\n",
    "\n",
    "# f = lambda x: x\n",
    "\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfca8a-e248-43bf-ab5e-c82afb2b8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(4)\n",
    "\n",
    "x_domain = np.linspace(-1, 1, 100)\n",
    "x_extension = np.linspace(-3, 3, 300)\n",
    "\n",
    "plt.plot(x_extension, S_N(f, N, x_extension), 'r', label='Fourier approx. on $\\mathbb{R}$')\n",
    "plt.plot(x_domain, f(x_domain), 'b', lw=2, label='f(x) on [-1,1]')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552eb47c-9c2b-4b12-aa01-6c82860177f4",
   "metadata": {},
   "source": [
    "**The Answer Lies Within.** Suppose that $f$ is a piecewise continuously differentiable function on $[-1,1]$. If $f$ is differentiable at $x\\in (-1,1)$ (note that $x$ is an interior point of the interval), then by adding and subtracting $f(x)$ to $f(y)$ and exploiting linearity of the integral, we have that\n",
    "\n",
    "$$\n",
    "    S_N(x) = \\underbrace{\\int_{-1}^1  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} (f(y)-f(x))\\, dy}_{\\mathbf{I}} + \\underbrace{\\int_{-1}^1  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} f(x)\\, dy}_{\\mathbf{II}}.\n",
    "$$\n",
    "\n",
    "The second integral, denoted by $\\mathbf{II}$, is equal to $f(x)$ because \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{II} &= f(x) \\int_{-1}^1  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} \\, dy \\\\\n",
    "                \\\\\n",
    "                &= f(x) \\left[\\int_{-1}^1 \\frac{1}{2} + \\sum_{k=1}^N \\cos(k\\pi(y-x))\\right] \\, dy \\\\\n",
    "                \\\\\n",
    "                &= f(x) \\underbrace{\\int_{-1}^1 \\frac{1}{2}\\, dy}_{=1} + f(x) \\underbrace{\\int_{-1}^1 \\sum_{k=1}^N \\cos(k\\pi(y-x))\\, dy}_{=0} \\\\\n",
    "                &= f(x).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the game has simplified to studying the limit of the integral denoted by $\\mathbf{I}$ above as $N\\to\\infty$. If we can show that the limit of this integral is zero as $N\\to\\infty$, then we will have proven that $S_N(x)\\to f(x)$ at this point of continuity $x\\in [-1,1]$. \n",
    "\n",
    "To simplify things, let $g(y)=f(y)-f(x)$ where $x$ is the fixed point of continuity under consideration and $y$ is free to vary in $[-1,1]$. Since $f$ is assumed continuous at $x$, then $g$ is continuous at $x$ and $g(x)=0$. Since $f$ is assumed piecewise differentiable on $[-1,1]$, then so is $g$. Now, we rewrite $\\mathbf{I}$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{I} &= \\int_{-1}^1  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} g(y)\\, dy \\\\\n",
    "               \\\\\n",
    "               &= \\int_{-1}^1  \\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)\\frac{g(y)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)}\\, dy\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e8d9b-34f7-48af-b4db-698524a97240",
   "metadata": {},
   "source": [
    "**Stream of Consciousness.** We therefore view $\\mathbf{I}$ as defining a type of Fourier coefficient of $\\frac{g(y)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)}$ and wish to use Lemma 3.2.1 (more precisely the remark given after the outline of its proof) to prove this converges to zero as $N\\to\\infty$. To do so, we need to first establish that this ratio defines a piecewise-continuous function.\n",
    "\n",
    "At $y=x$, the function $\\frac{g(y)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)}$ evaluates to $\\frac{0}{0}$, but the denominator is nonzero for any other $y\\in[-1,1]$ since $x\\in(-1,1)$ implies $|\\pi(y-x)/2| < \\pi$. We therefore have that this is a piecewise continuous function everywhere except possibly at $y=x$ where we need to establish that the one-sided limits exist. Conveniently, we can apply [L'Hôpital's rule](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule) to conclude the one-sided limits of the derivative exist at this point since the derivative of the denominator is a cosine function that is non-zero at $y=x$.\n",
    "This establishes that $\\frac{g(y)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)}$ is indeed a piecewise-continuously differentiable function. \n",
    "\n",
    "We have proven the following result.\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Theorem 3.2.1: Pointwise Convergence of Fourier Series, Part I (The Best of Times)\n",
    "\n",
    "Let $f$ be a piecewise-continuously differentiable function on $[-1,1]$ and suppose $x\\in (-1,1)$ is a point of continuity for $f$, then the Fourier series converges to $f(x)$ at $x$.\n",
    "\n",
    "---\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa947b1-682d-4ba2-9e8a-ad48819f9976",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Section 3.2.5: Convergence of Fourier Series (Part II: Begin Again)\n",
    "---\n",
    "\n",
    "What happens if $x\\in(-1,1)$ and $f$ has a jump discontinuity at this $x$?\n",
    "\n",
    "Do we begin all over again? NOOOOO! We just get clever.\n",
    "\n",
    "First of all, we recognize that if $f$ is replaced by its $2$-periodic continuation from $[-1,1]$ to $\\mathbb{R}$ and we redefine the integral for $S_N(x)$ over any interval of length $2$, then the answer will be the same. So, we consider the integral over $[x-1, x+1]$.\n",
    "\n",
    "Second, we break up the integral as the sum over two intervals $J^- = (x-1, x)$ and $J^+ = (x, x+1)$ and (pun intended) the **answer lies within** as before where we write\n",
    "\n",
    "$$\n",
    "    S_N(x) = S_N(x^-) + S_N(x^+)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    S_N(x^-) := \\int_{x-1}^x  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} (f(y)-f(x^-))\\, dy + \\int_{x-1}^x  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} f(x^-)\\, dy\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    S_N(x^+) := \\int_{x}^{x+1}  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} (f(y)-f(x^+))\\, dy + \\int_{x}^{x+1}  \\frac{\\sin\\left(\\left(N+\\frac{1}{2}\\right)\\pi(y-x)\\right)}{2\\sin\\left(\\frac{\\pi(y-x)}{2}\\right)} f(x^+)\\, dy.\n",
    "$$\n",
    "\n",
    "In the above integrals, $f(x^-):=\\lim_{y\\uparrow x} f(y)$ and $f(x^+):=\\lim_{y\\downarrow x} f(y)$.\n",
    "\n",
    "As in the prior case, Lemma 3.2.1 implies that the first two integrals involved in $S_N(x^-)$ and $S_N(x^+)$ have limits of zero as $N\\to\\infty$. Also, as in the prior case, we can simply factor out the $f(x^-)$ and $f(x^+)$, respectively, in the second integrals in $S_N(x^-)$ and $S_N(x^+)$, respectively. Unlike the prior case, these integrals are now only equal to $1/2$ instead of $1$. From this, we conclude that\n",
    "\n",
    "$$\n",
    "    S_N(x)\\to \\frac{1}{2} \\left(f(x^-)+f(x^+)\\right), \\ \\text{ as } \\ N\\to\\infty.\n",
    "$$\n",
    "\n",
    "In other words, at points of a jump discontinuity, the Fourier series converges to the \"average\" value of $f(x)$ at this point as defined by the average value of its one-sided limits.\n",
    "\n",
    "The technique above where we shift the integrals and utilize the $2$-periodic extension of $f$ can also be used to show that \n",
    "\n",
    "$$\n",
    "    S_N(\\pm 1) \\to \\frac{1}{2} \\left(f(-1^+)+f(1^-)\\right), \\ \\text{ as } N\\to\\infty.\n",
    "$$\n",
    "\n",
    "In other words, $S_N(-1)$ and $S_N(1)$ converge to the average values of the one-sided limits of $f$ at the endpoints of the interval $[-1,1]$.\n",
    "\n",
    "This **illumination theory** is summarized in the following result.\n",
    "\n",
    "---\n",
    "#### Theorem 3.2.2: Pointwise Convergence of Fourier Series, Part II (A Nightmare to Remember)\n",
    "\n",
    "Let $f$ be a piecewise-continuously differentiable function on $[-1,1]$. Then, the Fourier series converges to\n",
    "\n",
    "(a) $f(x)$ if $x\\in(-1,1)$ and $f$ is continuous at $x$; \n",
    "\n",
    "(b) $\\frac{1}{2} \\left(f(x^-)+f(x^+)\\right)$ if $x\\in(-1,1)$ and $f$ is discontinuous at $x$; \n",
    "\n",
    "(c) $\\frac{1}{2} \\left(f(-1^+) + f(1^-)\\right)$ if $x=-1$ or $x=1$.\n",
    "\n",
    "---\n",
    "<br><br>\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- Theorem 3.2.2(b) and Theorem 3.2.2(c) mathematically describe what is known as [***Gibbs phenomenon***](https://en.wikipedia.org/wiki/Gibbs_phenomenon).\n",
    "\n",
    "- The convergence of Theorem 3.2.2(a) (which is just a restatement of Theorem 3.2.1) and of Theorem 3.2.2(b) and Theorem 3.2.2(c) are *all* pointwise. But, it is possible to prove uniform convergence on sub-intervals $[a,b]$ that do not contain any jump discontinuities of the 2-periodic extension of $f$.\n",
    "\n",
    "- We still have not addressed convergence when $f$ is *only* piecwise-continuous. Ironically enough, we need to consider *smoother* $f$ before we consider *rougher* $f$ (i.e., $f$ that are *only* piecewise-continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3ec49-3bed-4092-920d-c49f0f5f0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x\n",
    "\n",
    "x_end_pts = np.array([-1, 1])\n",
    "\n",
    "Ns = [5, 10, 20, 40]\n",
    "\n",
    "S_Ns = np.zeros( (2, len(Ns)) )\n",
    "for i in range(len(Ns)):\n",
    "    S_Ns[:,i] = S_N(f, Ns[i], x_end_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95124b15-2977-4b65-9e2d-c654c310ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(f(x_end_pts)))\n",
    "print('-'*50)\n",
    "print(S_Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5baaa-91be-436a-896e-c1afd8a81f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3-2*x**2\n",
    "\n",
    "x_end_pts = np.array([-1, 1])\n",
    "\n",
    "Ns = [5, 10, 20, 40]\n",
    "\n",
    "S_Ns = np.zeros( (2, len(Ns)) )\n",
    "for i in range(len(Ns)):\n",
    "    S_Ns[:,i] = S_N(f, Ns[i], x_end_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dac16e-7a9e-4a4c-bdd9-bda9d08da06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(f(x_end_pts)))\n",
    "print('-'*50)\n",
    "print(S_Ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21daca-7e2c-43c8-80f0-f183b959f401",
   "metadata": {},
   "source": [
    "### Section 3.2.6: Uniform Convergence (Caught in a Web)\n",
    "---\n",
    "\n",
    "Suppose now that $f$ is actually continous and periodic with $f(-1)=f(1)$ so that its 2-periodic extension is also continuous and further assume that $f'$ is piecewise continuous on $[-1,1]$. \n",
    "\n",
    "In this case, Theorem 3.2.2 implies that for all $x\\in[-1,1]$, \n",
    "\n",
    "$$\n",
    "    f(x) = \\lim_{N\\to\\infty} S_N(x) = \\frac{a_0}{2} + \\sum_{k=1}^\\infty \\left(a_k\\cos(k\\pi x) + b_k\\sin(k\\pi x)\\right).\n",
    "$$\n",
    "\n",
    "This implies that for each $x\\in[-1,1]$, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    |f(x)-S_N(x)| &= \\left|  \\sum_{k=N+1}^\\infty \\left(a_k\\cos(k\\pi x) + b_k\\sin(k\\pi x)\\right)\\right| \\\\\n",
    "                  \\\\\n",
    "                  &\\leq \\sum_{k=N+1}^\\infty \\left(|a_k| + |b_k|\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Taking the supremum over $x\\in[-1,1]$ on both sides of the inequality (and observing that the right-hand side of the inequality is independent of $x$) gives\n",
    "\n",
    "$$\n",
    "    \\| f-S_N \\|_\\infty \\leq \\sum_{k=N+1}^\\infty \\left(|a_k| + |b_k|\\right).\n",
    "$$\n",
    "\n",
    "If we can show that the sum is finite, then for sufficiently large $N$, we can make this sum as small as we need to conclude that $S_N\\to f$ uniformly on $[-1,1]$. We do this by relating this sum to a Fourier series expansion of $f'$.\n",
    "\n",
    "By assumption of $f'$ being piecewise continuous, it has a Fourier series expansion that we denote by\n",
    "\n",
    "$$\n",
    "    \\frac{\\alpha_0}{2} + \\sum_{k=1}^\\infty \\left(\\alpha_k\\cos(k\\pi x) + \\beta_k\\sin(k\\pi x)\\right).\n",
    "$$\n",
    "\n",
    "By assumption of $f(-1)=f(1)$, we have that $\\alpha_0=0$. Utilizing [Bessel's inequality](https://en.wikipedia.org/wiki/Bessel%27s_inequality), we have that\n",
    "\n",
    "$$\n",
    "    \\sum_{k=1}^\\infty \\left(\\alpha_k^2 + \\beta_k^2 \\right) \\leq \\int_{-1}^1 (f'(x))^2\\, dx <\\infty.\n",
    "$$\n",
    "\n",
    "At the same time, an integration by parts shows that $\\alpha_k=k\\pi b_k$ and $\\beta_k=-k\\pi a_k$, so for any integer $N$, we have that\n",
    "\n",
    "$$\n",
    "    \\sum_{k=1}^N \\left(|a_k| + |b_k|\\right) = \\frac{1}{\\pi} \\sum_{k=1}^N \\frac{1}{k} \\left( |\\alpha_k| + |\\beta_k| \\right).\n",
    "$$\n",
    "\n",
    "Now, a [Cauchy-Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality) implies that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\sum_{k=1}^N \\frac{1}{k}|a_k| &\\leq \\left(\\sum_{k=1}^N \\frac{1}{k^2}\\right)^{1/2}\\left(\\sum_{k=1}^N \\alpha_k^2\\right)^{1/2} \\\\\n",
    "                                  \\\\\n",
    "                                  &\\leq \\left(\\sum_{k=1}^N \\frac{1}{k^2}\\right)^{1/2}\\left(\\sum_{k=1}^N \\alpha_k^2+\\beta_k^2\\right)^{1/2}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice we get the same inequality for $\\sum{k=1}^N \\frac{1}{k}|b_k|$. This implies that\n",
    "\n",
    "$$\n",
    "    \\sum_{k=1}^N \\left(|a_k| + |b_k|\\right) \\leq \\frac{2}{\\pi} \\left(\\sum_{k=1}^N \\frac{1}{k^2}\\right)^{1/2}\\left(\\sum_{k=1}^N \\alpha_k^2+\\beta_k^2\\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "Since this bound holds for all $N$ and the terms involved are all nonnegative, we have that\n",
    "\n",
    "$$\n",
    "    \\sum_{k=1}^\\infty \\left(|a_k| + |b_k|\\right) \\leq \\frac{2}{\\pi} \\left(\\sum_{k=1}^\\infty \\frac{1}{k^2}\\right)^{1/2}\\left(\\sum_{k=1}^\\infty \\alpha_k^2+\\beta_k^2\\right)^{1/2} \n",
    "$$\n",
    "\n",
    "where we understand this may mean $\\infty\\leq \\infty$, which is perfectly acceptable. However, we will in fact show that this implies the sum on the left-hand side of the inequality is finite.\n",
    "\n",
    "From a [$p$-series test](https://en.wikipedia.org/wiki/Convergence_tests#p-series_test), \n",
    "\n",
    "$$\n",
    "    \\left(\\sum_{k=1}^\\infty \\frac{1}{k^2}\\right)^{1/2}<\\infty.\n",
    "$$\n",
    "\n",
    "Using the previous bound from Bessel's inequality, \n",
    "\n",
    "$$\n",
    "    \\left(\\sum_{k=1}^\\infty \\alpha_k^2+\\beta_k^2\\right)^{1/2} \\leq \\left(\\int_{-1}^1 (f'(x))^2\\, dx\\right)^{1/2} < \\infty.\n",
    "$$\n",
    "\n",
    "Putting this all together, we have that\n",
    "\n",
    "$$\n",
    "    \\sum_{k=1}^\\infty \\left(|a_k| + |b_k|\\right) <\\infty.\n",
    "$$\n",
    "\n",
    "The uniform convergence is thus proven.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Theorem 3.2.3: Uniform Convergence of Fourier Series (Just Let me Breathe)\n",
    "\n",
    "Let $f$ be continous and periodic with $f(-1)=f(1)$ so that its 2-periodic extension is also continuous and further assume that $f'$ is piecewise continuous on $[-1,1]$. Then, the Fourier series converges uniformly to $f$ on $[-1,1]$ and the Fourier coefficients converge to zero at a rate of $\\mathcal{O}(1/k)$. \n",
    "\n",
    "---\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50528ad-b2f8-4ce1-b235-3f99bd9dc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.heaviside(-x, 0)*(1+x) + np.heaviside(x, 1)*(1-x)\n",
    "\n",
    "%matplotlib widget\n",
    "plt.figure(5)\n",
    "\n",
    "x_domain = np.linspace(-1, 1, 100)\n",
    "\n",
    "plt.plot(x_domain, f(x_domain), 'b', lw=2, label='f(x) on [-1,1]')\n",
    "\n",
    "Ns = [2, 4, 8, 16, 32]\n",
    "\n",
    "for i in range(len(Ns)):\n",
    "    plt.plot(x_domain, S_N(f, Ns[i], x_domain), ls='--', label='$S_{}' + str(Ns[i]) + '}(x)$')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b2135-b1bc-452e-8241-edf08b69d3a4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Section 3.2.7: Mean Square Convergence (A New Beginning)\n",
    "---\n",
    "\n",
    "We can now weaken the smoothness of $f$ by just assuming it is piecewise-continuous to get the following result.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Theorem 3.2.4: $L^2$ Convergence of Fourier Series (Cover My Eyes)\n",
    "\n",
    "Let $f$ be a piecewise-continuous function on $[-1,1]$. Then, the Fourier series converges to $f$ in the $L^2$ (i.e., mean square) sense. \n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "***Idea of proof of Theorem 3.2.4:***\n",
    "\n",
    "1. Let $f$ refer to its 2-periodic extension on $\\mathbb{R}$ and for each $x\\in [-1,1]$ and $\\delta>0$, define $f_\\delta(x):=\\frac{1}{2\\delta} \\int_{x-\\delta}^{x+\\delta} f(y)\\, dy$, which is the average value of $f$ on $[x-\\delta, x+\\delta]$. \n",
    "\n",
    "2. Recognize that $f_\\delta$ is 2-periodic (because $f$ is recognized as its 2-periodic extension), and by the fundamental theorem of calculus $f_\\delta'$ is piecewise continuous and $f_\\delta$ itself is continuous. \n",
    "\n",
    "3. Recognize that Theorem 3.2.3 applies to $f_\\delta$.\n",
    "\n",
    "4. Letting $S_N(x)$ denote the Fourier partial sum of $f$ and $S_{N,\\delta}(x)$ denote the Fourier partial sum of $f_\\delta$, write\n",
    "\n",
    "$$\n",
    "    S_N(x) - f(x) = \\underbrace{S_N(x)-S_{N,\\delta}(x)}_{\\mathbf{I}} + \\underbrace{S_{N,\\delta}(x)-f_\\delta(x)}_{\\mathbf{II}} + \\underbrace{f_\\delta(x)-f(x)}_{\\mathbf{III}}.\n",
    "$$\n",
    "\n",
    "5. Apply the $L^2$ norm (i.e., mean square error) and a triangle inequality to get a bound on the $L^2$ norm of $S_n(x)-f(x)$ in terms of the sums of the $L^2$ norms of terms $\\mathbf{I}$, $\\mathbf{II}$, and $\\mathbf{III}$.\n",
    "\n",
    "6. Use different arguments to prove that these $L^2$ norms of $\\mathbf{I}$, $\\mathbf{II}$, and $\\mathbf{III}$ can all be made small to finish the proof.\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb1df8-f472-45c9-9179-08526b2200e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "5//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88d020-198b-4e10-9761-37f4b828eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_x(x):\n",
    "    if int(x)%2 == 0:\n",
    "        return x-int(x)\n",
    "    else:\n",
    "        return -1+np.abs(x)-int(np.abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac4a2e-eac5-45cf-81c7-429701bf94a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_x(-1.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e9edc-24b1-4df4-8b8e-0b5c48cbceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2_periodic(f, x):\n",
    "    if int(x)%2 == 0:\n",
    "        return f(x-int(x))\n",
    "    else:\n",
    "        return f(-1+x-int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc6be9-513c-463e-9ddc-d8167f90a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_delta(f, delta, x):\n",
    "    v = np.zeros(len(x))\n",
    "    f_2 = lambda x: f_2_periodic(f, x)\n",
    "    for i in range(len(x)):\n",
    "        v[i] = 1/(2*delta)*quad(f_2, x[i]-delta, x[i]+delta)[0]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb0352-e3e9-435c-b2a8-7e78185e5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.heaviside(-x, 0)*(1+x) + np.heaviside(x, 1)*(1-x)\n",
    "\n",
    "%matplotlib widget\n",
    "plt.figure(6)\n",
    "\n",
    "x_domain = np.linspace(-1, 1, 100)\n",
    "\n",
    "deltas = [0.2, 0.1, 0.05, 0.025]\n",
    "\n",
    "x_extension = np.linspace(-3, 3, 300)\n",
    "\n",
    "for i in range(len(deltas)):\n",
    "    plt.plot(x_extension, f_delta(f, deltas[i], x_extension), ls='--', label='$f_{' + str(deltas[i]) + '}(x)$')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)\n",
    "\n",
    "plt.plot(x_domain, f(x_domain), 'b', lw=2, label='f(x) on [-1,1]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07344f-eec7-4224-a6b1-932fa5605fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3-2*x**2\n",
    "\n",
    "%matplotlib widget\n",
    "plt.figure(7)\n",
    "\n",
    "x_domain = np.linspace(-1, 1, 100)\n",
    "\n",
    "deltas = [0.2, 0.1, 0.05, 0.025, 0.01, 0.005]\n",
    "\n",
    "x_extension = np.linspace(-3, 3, 300)\n",
    "\n",
    "for i in range(len(deltas)):\n",
    "    plt.plot(x_extension, f_delta(f, deltas[i], x_extension), ls='--', label='$f_{' + str(deltas[i]) + '}(x)$')\n",
    "\n",
    "plt.legend(fontsize=12, shadow=True)\n",
    "\n",
    "plt.plot(x_domain, f(x_domain), 'b', lw=2, label='f(x) on [-1,1]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39c7b6-96aa-425d-824d-1e5932c0a310",
   "metadata": {},
   "source": [
    "### Section 3.2.8: Rates of Convergence for Smooth Functions (Build Me Up, Break Me Down)\n",
    "---\n",
    "\n",
    "**[Troy's note to Troy] Unfinished work. Just focus on takeaways. Finish over summer.**\n",
    "\n",
    "The main takeaways of this final part of the notebook are that\n",
    "\n",
    "- the smoother $f$ is, the faster the associated Fourier coefficients of $f$ decay to zero, and\n",
    "\n",
    "- the faster the Fourier coefficients decay to zero, the smoother the function $f$ must be that is the limit of the associated Fourier series.\n",
    "\n",
    "The proofs use induction based on the argument shown in Section 3.2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e6f4b-3ff0-4c41-801a-125f93a0a0d4",
   "metadata": {},
   "source": [
    "**Some notation**\n",
    "\n",
    "- Let $\\mathcal{C}_p^m((-1,1))$ denote the functions that are $m$-times continuously differentiable on $(-1,1)$ and 2-periodic so that $f^{(j)}(-1)=f^{(j)}(1)$ for $j=0,1,\\ldots, m$ for all $f\\in\\mathcal{C}_p^m((_1,1))$.\n",
    "\n",
    "- Let $\\widehat{\\mathcal{C}_p^m}((-1,1))$ denote the functions that are in $\\mathcal{C}_p^{m-1}((-1,1))$ with $f^{(m)}$ being piecewise continuous.\n",
    "\n",
    "The goal is to generalize the results from Section 3.2.6 where we only assumed $f'$ was piecewise continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cb0c2-3a25-4863-8587-a187511b15ac",
   "metadata": {},
   "source": [
    "**An inductive argument**\n",
    "\n",
    "Following the analysis of Section 3.2.6, we treat Theorem 3.2.3 as the base case for $f\\in\\widehat{\\mathcal{C}_p^1}((-1,1))$.\n",
    "\n",
    "Assume for the first $m$ cases that if $f\\in\\widehat{\\mathcal{C}_p^m}((-1,1))$, then the Fourier series converges uniformly to $f$ on $[-1,1]$ and the Fourier coefficients converge to zero at a rate of $\\mathcal{O}(1/k^m)$.\n",
    "\n",
    "For the $m+1$ case, if $f\\in\\widehat{\\mathcal{C}_p^m}((-1,1))$, then $f'\\in\\widehat{\\mathcal{C}_p^m}((-1,1))$, and the relationship of $\\alpha_k=k\\pi b_k$ and $\\beta_k=-k\\pi a_k$ (where $\\alpha_k$ and $\\beta_k$ denote the Fourier coefficients of $f'$ while $a_k$ and $b_k$ denote the Fourier coefficients of $f$) gives the result that the Fourier series converges uniformly to $f$ on $[-1,1]$ and that the Fourier coefficients converge to zero at a rate of $\\mathcal{O}(1/k)\\mathcal{O}(1/k^m)=\\mathcal{O}(1/k^{m+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde465c4-1c6f-4014-9980-9d32e12fb9e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f5b4a-3da6-435b-af11-35de40f21bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
